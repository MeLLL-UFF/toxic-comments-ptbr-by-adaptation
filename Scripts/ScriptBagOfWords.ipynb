{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75b28d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python3 -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e320365",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Criando dataset de treino e de teste ###\n",
    "dataset_number = 1\n",
    "\n",
    "# Lacafe\n",
    "text = 'txt'\n",
    "label = 'has_anger'\n",
    "train_data = pd.read_csv(f'./Datasets/Lacafe/df_dataset_train_{dataset_number}.csv')\n",
    "test_data = pd.read_csv(f'./Datasets/Lacafe/df_dataset_test_{dataset_number}.csv')\n",
    "\n",
    "# Fortuna (Updated)\n",
    "# text = 'text'\n",
    "# label = 'hatespeech_comb'\n",
    "# train_data = pd.read_csv(f'./Datasets/FortunaUpdated/2019-05-28_portuguese_hate_speech_binary_classification_train_{dataset_number}.csv')\n",
    "# test_data = pd.read_csv(f'./Datasets/FortunaUpdated/2019-05-28_portuguese_hate_speech_binary_classification_test_{dataset_number}.csv')\n",
    "\n",
    "# OffComBr\n",
    "# text = 'text'\n",
    "# label = 'offensive'\n",
    "# train_data = pd.read_csv(f'./Datasets/OffComBR/OffComBR2_train_{dataset_number}.csv')\n",
    "# test_data = pd.read_csv(f'./Datasets/OffComBR/OffComBR2_test_{dataset_number}.csv')\n",
    "\n",
    "# HateBR\n",
    "# text = 'instagram_comments'\n",
    "# label = 'offensive_language'\n",
    "# train_data = pd.read_csv(f'./Datasets/HateBR/HateBR_train_{dataset_number}.csv')\n",
    "# test_data = pd.read_csv(f'./Datasets/HateBR/HateBR_test_{dataset_number}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd49ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Criando a lista de pontuações\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Criando a lista de stop words\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "\n",
    "# Criando a função de tokenização\n",
    "def spacy_tokenizer(sentence, lemmatize=False, remove_stop_words=False, remove_punctuations=False):\n",
    "    # Criando a lista de tokens\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lematizando os tokens e colocando em caixa baixa\n",
    "    if (lemmatize):\n",
    "        mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "    else:\n",
    "        mytokens = [ word.text.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # Removendo stop words\n",
    "    if (remove_stop_words):\n",
    "        mytokens = [ word for word in mytokens if word not in stop_words ]        \n",
    "        \n",
    "    if (remove_punctuations):\n",
    "        mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "        \n",
    "    # Retornando a lista de token reprocessado\n",
    "    return mytokens\n",
    "\n",
    "text_pipeline = lambda x: spacy_tokenizer(x, lemmatize=True, remove_stop_words=True)\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizando, transformando para lemas e botando em caixa baixa cada tweet\n",
    "train_data[text] = [text_pipeline(str(txt)) for txt in train_data[text]]\n",
    "test_data[text] = [text_pipeline(str(txt)) for txt in test_data[text]]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349624a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Juntando os tokens de volta em um única string\n",
    "train_data[text] = train_data.apply(lambda x: \" \".join(x[text]), axis=1)\n",
    "test_data[text] = test_data.apply(lambda x: \" \".join(x[text]), axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando o treino e o teste em dois conjuntos diferentes\n",
    "x_train = train_data[text]\n",
    "y_train = train_data[label]\n",
    "x_test = test_data[text]\n",
    "y_test = test_data[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo em BOW com valoração de frequência\n",
    "freq_vector = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(train_data[text])\n",
    "\n",
    "x_train = freq_vector.transform(x_train)\n",
    "x_test = freq_vector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1031516",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier sem Cross Validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(max_iter=500)\n",
    "\n",
    "# model generation\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados do dataset de treino\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = classifier.predict(x_train)\n",
    "metrics = precision_recall_fscore_support(y_train, y_pred_train, average='macro')\n",
    "accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print('---- Dataset de treino ----')\n",
    "print(f'Accuracy: {str(accuracy * 100).replace(\".\", \",\")}%')\n",
    "print(f'Precision: {str(metrics[0] * 100).replace(\".\", \",\")}%')\n",
    "print(f'Recall: {str(metrics[1] * 100).replace(\".\", \",\")}%')\n",
    "print(f'Fscore: {str(metrics[2] * 100).replace(\".\", \",\")}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d3610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados do dataset de teste\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "metrics = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('---- Dataset de teste ----')\n",
    "print(f'Accuracy: {str(accuracy * 100).replace(\".\", \",\")}%')\n",
    "print(f'Precision: {str(metrics[0] * 100).replace(\".\", \",\")}%')\n",
    "print(f'Recall: {str(metrics[1] * 100).replace(\".\", \",\")}%')\n",
    "print(f'Fscore: {str(metrics[2] * 100).replace(\".\", \",\")}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando matriz de confusão de teste\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "def plot_cm(conf_matrix):\n",
    "  sns.set(font_scale=1.4,color_codes=True,palette=\"deep\")\n",
    "  sns.heatmap(cm,annot=True,annot_kws={\"size\":16},fmt=\"d\",cmap=\"YlGnBu\")\n",
    "  plt.title(\"Confusion Matrix\")\n",
    "  plt.xlabel(\"Predicted Value\")\n",
    "  plt.ylabel(\"True Value\")\n",
    "\n",
    "plot_cm(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
