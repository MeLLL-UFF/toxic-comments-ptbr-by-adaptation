{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext spacy\n",
    "# python3 -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l_rvQMf-Jq5g"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, drop_prob, hs1, num_class, freeze=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        if freeze:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(embed_dim, hs1) \n",
    "        self.fc2 = nn.Linear(hs1, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = embedded.view(embedded.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        preds = self.softmax(self.fc2(x))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ew8i1QL2SRVU"
   },
   "outputs": [],
   "source": [
    "text = data.Field()\n",
    "\n",
    "# Criando dataset de treino e de teste\n",
    "data = data.TabularDataset(path=\"df_dataset.csv\",\n",
    "                                 format=\"csv\",\n",
    "                                 fields=[\n",
    "                                        ('', text),\n",
    "                                        ('', text),\n",
    "                                        ('has_anger', data.Field()),\n",
    "                                        ('', text),\n",
    "                                        ('txt', text)\n",
    "                                 ],\n",
    "                                 skip_header=True)\n",
    "\n",
    "# train_data = data.TabularDataset(path=\"df_dataset.csv\",\n",
    "#                                  format=\"csv\",\n",
    "#                                  fields=[\n",
    "#                                         ('', text),\n",
    "#                                         ('', text),\n",
    "#                                         ('has_anger', data.Field()),\n",
    "#                                         ('', text),\n",
    "#                                         ('txt', text)\n",
    "#                                  ],\n",
    "#                                  skip_header=True)\n",
    "\n",
    "# test_data = data.TabularDataset(path=\"df_dataset_test.csv\",\n",
    "#                                 format=\"csv\",\n",
    "#                                 fields=[\n",
    "#                                         ('', text),\n",
    "#                                         ('', text),\n",
    "#                                         ('has_anger', data.Field()),\n",
    "#                                         ('', text),\n",
    "#                                         ('txt', text)\n",
    "#                                 ],\n",
    "#                                 skip_header=True)\n",
    "\n",
    "#text.build_vocab(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7251"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gz3HmjcvKFEo"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "#PAD_IDX = text.vocab.stoi[text.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SYWhXINHnYrk"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25002\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 8\n",
    "EMBED_DIM = 300\n",
    "NUM_CLASS = 2\n",
    "DROPOUT_PROB = 0.5\n",
    "HS1 = 128\n",
    "\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, DROPOUT_PROB, HS1, NUM_CLASS, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jLuNJU-dS0Of"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 3.5951, -0.9948, -2.4219,  ...,  2.0744,  0.1685, -2.3501],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "pretrained_embeddings = vocab.Vectors(name = 'nilc_embeddings/fasttext_cbow_s300.txt')\n",
    "\n",
    "text.build_vocab(data,\n",
    "#                  train_data, \n",
    "#                  test_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = pretrained_embeddings,\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "model.embedding.weight.data.copy_(text.vocab.vectors)\n",
    "# UNK_IDX = text.vocab.stoi[text.unk_token]\n",
    "\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "#model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)\n",
    "\n",
    "#print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence, lemmatize=False, remove_stop_words=False, remove_punctuations=False):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lematizando os tokens e colocando em caixa baixa\n",
    "    if (lemmatize):\n",
    "        mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "    else:\n",
    "        mytokens = [ word.text.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # Removendo stop words\n",
    "    if (remove_stop_words):\n",
    "        mytokens = [ word for word in mytokens if word not in stop_words ]        \n",
    "        \n",
    "    if (remove_punctuations):\n",
    "        mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "        \n",
    "    # Retornando a lista de token reprocessado\n",
    "    return mytokens\n",
    "\n",
    "text_pipeline = lambda x: spacy_tokenizer(x, lemmatize=True, remove_stop_words=True)\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w6tRmuvdO1Ig"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([label_pipeline(entry.has_anger[0]) for entry in batch])\n",
    "    _text = []\n",
    "    for entry in batch:\n",
    "        _entry = []\n",
    "        entry_txt = text_pipeline(\"\".join(entry.txt))\n",
    "        for t in entry_txt:\n",
    "            _entry.append(text.vocab.stoi[t])\n",
    "        _text.append(torch.tensor(_entry,dtype=torch.long))\n",
    "    offsets = [0] + [len(entry) for entry in _text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    _text = torch.cat(_text)\n",
    "    #print(' BATCH' , _text)\n",
    "    #print(offsets)\n",
    "    return _text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_HebyntEO56B"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        #output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            #output = model(text)\n",
    "            #print(output, cls)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uendMPTfPBNx",
    "outputId": "3f1ef8f4-8b90-4e7b-a63f-510808c8be36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f74a5de8cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "N_EPOCHS = 1\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "#optimizer = optim.SparseAdam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "# Divide 90% para o treino e 10% para os testes\n",
    "train_len = int(len(data) * 0.90)\n",
    "sub_train_, sub_test_ = random_split(data, [train_len, len(data) - train_len])\n",
    "sub_train_\n",
    "\n",
    "# Do que sobrou do treino, 95% para o treino e 5% para a validacao\n",
    "train_len = int(len(sub_train_) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(sub_train_, [train_len, len(sub_train_) - train_len])\n",
    "sub_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5prB___LVkxm",
    "outputId": "239eeca3-5aff-4259-b1f9-9b6c008f8c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, | time in 1.65 minutes and 39 seconds\n",
      "\tLoss: 0.0617(train)\t|\tAcc: 82.0%(train)\n",
      "\tLoss: 0.0028(valid)\t|\tAcc: 83.2%(valid)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1}, | time in {mins} minutes and {secs} seconds')\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlrzrwbBWLtj",
    "outputId": "b25c85cd-5c1d-4429-b432-1f93454b16e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0013(test)\t|\tAcc: 83.3%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(sub_test_)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y31fmkrvF7kD"
   },
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Return an iterator that yields the given tokens and their ngrams.\n",
    "\n",
    "    Arguments:\n",
    "        token_list: A list of tokens\n",
    "        ngrams: the number of ngrams.\n",
    "\n",
    "    Examples:\n",
    "        >>> token_list = ['here', 'we', 'are']\n",
    "        >>> list(ngrams_iterator(token_list, 2))\n",
    "        >>> ['here', 'here we', 'we', 'we are', 'are']\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_ngrams(n):\n",
    "        return zip(*[token_list[i:] for i in range(n)])\n",
    "\n",
    "    for x in token_list:\n",
    "        yield x\n",
    "    for n in range(2, ngrams + 1):\n",
    "        for x in _get_ngrams(n):\n",
    "            yield ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmtbOGHTFQXh",
    "outputId": "e833cbcf-82e8-44b7-a5a7-92c91f61d275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertos: 604\n",
      "Total: 726\n",
      "Acurácia: 83.1956%\n",
      "Precision: 0.7555%\n",
      "Recall: 0.9689%\n",
      "Fscore: 0.8490%\n"
     ]
    }
   ],
   "source": [
    "NGRAMS = 2\n",
    "# label = {0 : \"N\",\n",
    "#          1 : \"S\"}\n",
    "\n",
    "def predict(_text, model, vocab, ngrams):\n",
    "    if len(_text) == 0:\n",
    "        return 0\n",
    "    with torch.no_grad():\n",
    "        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n",
    "        output = model(torch.tensor(_text), torch.tensor([0]))\n",
    "        return output.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')\n",
    "\n",
    "# Rodando os testes e obtendo dados da matriz de confusao para calculo das metricas\n",
    "cm = {'true_positive': 0, 'true_negative': 0, 'false_positive': 0, 'false_negative': 0}\n",
    "for entry in sub_test_:\n",
    "    if (predict(text_pipeline(\"\".join(entry.txt)), model, text.vocab, NGRAMS) == int(entry.has_anger[0])):\n",
    "        if int(entry.has_anger[0]) == 1:\n",
    "            cm['true_positive'] += 1\n",
    "        else:\n",
    "            cm['true_negative'] += 1\n",
    "    else:\n",
    "        if int(entry.has_anger[0]) == 0:\n",
    "            cm['false_positive'] += 1\n",
    "        else:\n",
    "            cm['false_negative'] += 1\n",
    "\n",
    "# Metricas\n",
    "total = len(sub_test_)\n",
    "precision = cm['true_positive'] / (cm['true_positive'] + cm['false_positive'])\n",
    "recall = cm['true_positive'] / (cm['true_positive'] + cm['false_negative'])\n",
    "fscore = 2 * ((precision * recall) / (precision + recall))\n",
    "acertos = cm['true_positive'] + cm['true_negative']\n",
    "print(f'Acertos: {cm[\"true_positive\"] + cm[\"true_negative\"]}')\n",
    "print(f'Total: {total}')\n",
    "print(f'Acurácia: {(acertos / total) * 100:.4f}%')\n",
    "print(f'Precision: {precision:.4f}%')\n",
    "print(f'Recall: {recall:.4f}%')\n",
    "print(f'Fscore: {fscore:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "6.2-classificao-neural-pretrain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
