{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext spacy\n",
    "# python3 -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l_rvQMf-Jq5g"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, drop_prob, hs1, num_class, freeze=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        if freeze:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(embed_dim, hs1) \n",
    "        self.fc2 = nn.Linear(hs1, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = embedded.view(embedded.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        preds = self.softmax(self.fc2(x))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ew8i1QL2SRVU"
   },
   "outputs": [],
   "source": [
    "text = data.Field()\n",
    "\n",
    "# Criando dataset de treino e de teste\n",
    "train_data = data.TabularDataset(path=\"df_dataset.csv\",\n",
    "                                 format=\"csv\",\n",
    "                                 fields=[\n",
    "                                        ('', text),\n",
    "                                        ('', text),\n",
    "                                        ('has_anger', data.Field()),\n",
    "                                        ('', text),\n",
    "                                        ('txt', text)\n",
    "                                 ],\n",
    "                                 skip_header=True)\n",
    "\n",
    "test_data = data.TabularDataset(path=\"df_dataset_test.csv\",\n",
    "                                format=\"csv\",\n",
    "                                fields=[\n",
    "                                        ('', text),\n",
    "                                        ('', text),\n",
    "                                        ('has_anger', data.Field()),\n",
    "                                        ('', text),\n",
    "                                        ('txt', text)\n",
    "                                ],\n",
    "                                skip_header=True)\n",
    "\n",
    "#text.build_vocab(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7251"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gz3HmjcvKFEo"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "#PAD_IDX = text.vocab.stoi[text.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SYWhXINHnYrk"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25002\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 8\n",
    "EMBED_DIM = 300\n",
    "NUM_CLASS = 2\n",
    "DROPOUT_PROB = 0.5\n",
    "HS1 = 128\n",
    "\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, DROPOUT_PROB, HS1, NUM_CLASS, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jLuNJU-dS0Of"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 3.5951, -0.9948, -2.4219,  ...,  2.0744,  0.1685, -2.3501],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "pretrained_embeddings = vocab.Vectors(name = 'nilc_embeddings/fasttext_cbow_s300.txt')\n",
    "\n",
    "text.build_vocab(train_data, \n",
    "                 test_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = pretrained_embeddings,\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "model.embedding.weight.data.copy_(text.vocab.vectors)\n",
    "# UNK_IDX = text.vocab.stoi[text.unk_token]\n",
    "\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "#model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)\n",
    "\n",
    "#print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence, lemmatize=False, remove_stop_words=False, remove_punctuations=False):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lematizando os tokens e colocando em caixa baixa\n",
    "    if (lemmatize):\n",
    "        mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "    else:\n",
    "        mytokens = [ word.text.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # Removendo stop words\n",
    "    if (remove_stop_words):\n",
    "        mytokens = [ word for word in mytokens if word not in stop_words ]        \n",
    "        \n",
    "    if (remove_punctuations):\n",
    "        mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "        \n",
    "    # Retornando a lista de token reprocessado\n",
    "    return mytokens\n",
    "\n",
    "text_pipeline = lambda x: spacy_tokenizer(x, lemmatize=True, remove_stop_words=True)\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w6tRmuvdO1Ig"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([label_pipeline(entry.has_anger[0]) for entry in batch])\n",
    "    _text = []\n",
    "    for entry in batch:\n",
    "        _entry = []\n",
    "        entry_txt = text_pipeline(\"\".join(entry.txt))\n",
    "        for t in entry_txt:\n",
    "            _entry.append(text.vocab.stoi[t])\n",
    "        _text.append(torch.tensor(_entry,dtype=torch.long))\n",
    "    offsets = [0] + [len(entry) for entry in _text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    _text = torch.cat(_text)\n",
    "    #print(' BATCH' , _text)\n",
    "    #print(offsets)\n",
    "    return _text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_HebyntEO56B"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        #output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            #output = model(text)\n",
    "            #print(output, cls)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uendMPTfPBNx",
    "outputId": "3f1ef8f4-8b90-4e7b-a63f-510808c8be36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f3f4c0b9730>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "N_EPOCHS = 1\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "#optimizer = optim.SparseAdam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_data) * 0.95)\n",
    "print(train_len)\n",
    "sub_train_, sub_valid_ = random_split(train_data, [train_len, len(train_data) - train_len])\n",
    "sub_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5prB___LVkxm",
    "outputId": "239eeca3-5aff-4259-b1f9-9b6c008f8c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, | time in 0.5333333333333333 minutes and 32 seconds\n",
      "\tLoss: 0.0613(train)\t|\tAcc: 82.4%(train)\n",
      "\tLoss: 0.0017(valid)\t|\tAcc: 82.6%(valid)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1}, | time in {mins} minutes and {secs} seconds')\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlrzrwbBWLtj",
    "outputId": "b25c85cd-5c1d-4429-b432-1f93454b16e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0034(test)\t|\tAcc: 83.4%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_data)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y31fmkrvF7kD"
   },
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Return an iterator that yields the given tokens and their ngrams.\n",
    "\n",
    "    Arguments:\n",
    "        token_list: A list of tokens\n",
    "        ngrams: the number of ngrams.\n",
    "\n",
    "    Examples:\n",
    "        >>> token_list = ['here', 'we', 'are']\n",
    "        >>> list(ngrams_iterator(token_list, 2))\n",
    "        >>> ['here', 'here we', 'we', 'we are', 'are']\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_ngrams(n):\n",
    "        return zip(*[token_list[i:] for i in range(n)])\n",
    "\n",
    "    for x in token_list:\n",
    "        yield x\n",
    "    for n in range(2, ngrams + 1):\n",
    "        for x in _get_ngrams(n):\n",
    "            yield ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmtbOGHTFQXh",
    "outputId": "e833cbcf-82e8-44b7-a5a7-92c91f61d275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:  S debater com luquistajá cansei de bater palma pra maluco dançar.  Predito:  S\n",
      "Real:  S dependendo do objetivo e da sua periodização de treino, pode ser interessante. pelo preço, é interessante. não compraria, pois sou desconfiado com a midway.anão, siga as recomendações do cardiologista que lhe pareceu mais sensato. dificilmente as diretrizes recomendam repouso absoluto para cardiopatas, especialmente no seu caso. retorne ao seu cardio e fale que tem interesse em voltar à academia. procure um educador físico bom e retorne gradualmente aos treinos, com o acompanhamento do seu cardio. metabolismo basal + kcal/dia. lurke um pouco neste fio e verá algumas recomendações sobre como realizar os cálculos e distribuir as calorias por macro nutriente.dieta adequada, treino bom é suficiente. se tiver interesse (e grana), invista em uma suplementação boa. a creatina é um excelente suplemento para o ganho de força. pesquise sobre.anão, tenha em mente que o processo de emagrecimento (com ou sem perda de gordura) acontece por meio de um déficit calórico. na equação, se a sua ingesta de calorias se mantém, você vai precisar gastar mais kcals para poder ter um saldo calórico negativo ao final do dia, correto?a sua preparação deve ser, prioritariamente, voltada para seu desempenho na luta. desempenho na luta deve ser construído, por sua vez, sobre dois pilares, fisicamente falando: força (seja em treinamentos resistidos ou de explosão) e no condicionamento (considerando que você terá rounds na sua luta, terá de ter gás para, no mínimo, rounds).o planejamento técnico você provavelmente já está fazendo com sua equipe, mas aconselho que dê ênfase aos treinamentos de explosão e de condicionamento de luta. na academia (treinamento com pesos), dê enfase a treinamentos de força, converse com um educador físico de sua equipe e inicie o quanto antes. agora quanto à cozinha, dê uma pesquisada sobre nutrient timing e tente maximizar os seus ganhos de força por agora. acrescente mais um aeróbico a sua rotina diária (condicionamento) e distribua suas refeições de acordo com seus horários e suas necessidades. o fato de sua categoria não requerer um corte de peso lhe beneficia muito, visto que o déficit alimentar na fase de preparação é forniquendo para o lutador.se você aceita um relato de caso, eu era competidor de jiu jitsu até certo tempo atrás. apesar de as bases fisiológicas das modalidades serem diferentes, a preparação mantém-se praticamente a mesma no jiu jitsu e no boxe, porém, por eu precisar fazer corte de peso, me sentia prejudicado no ganho de força.pica relatada  Predito:  S\n",
      "Real:  S auto-boicotador reportando dentro. passei quase anos para me formar num curso que não precisava mais de , e levou muito tempo para eu entender por que me sabotava tanto. tive depressão fudida, tomei remédios que ajudaram muito pouco, acabei indo pra terapia e só então houve algum progresso. sei que falam muito que terapia é pescaria, mas é importante lembrarmos que existem profissionais bons e ruins em qualquer área.no final das contas, sempre existe um motivo emocional profundo que dificilmente conseguimos identificar sem um bocado de terapia (ou amigos muito íntimos e perspicazes). no meu caso, resumindo absurdamente, tem a ver com um medo de desfazer um relacionamento de co-dependência com uma mãe controladora e obsessiva, e a busca pela aprovação de um pai ausente, tudo isso culminando na minha graduação. óbvio que vai ser diferente pra cada pessoa, mas sempre são conflitos que corroem a nossa vontade de dedicar qualquer esforço a uma tarefa que possa mudar esse status quo.procurem ajuda profissional, anões, e certamente vocês vão descobrir o centro dos seus problemas. a partir daí, vocês vão saber o que realmente precisam combater para acabar com esses ciclos de merda.  Predito:  S\n",
      "Real:  N tirei nota baixa na prova q jurava tirar nota alta, vai se foderrr  Predito:  N\n",
      "Real:  S also, recomendo o livro de lutero sobre os judeus e suas mentiras.lutero odiava mais judeus do que hitler jamais odiou.  Predito:  S\n",
      "Real:  S não se importe com o like, ou retorno, apenas tente convencer todas a dar para você, das feias até as bonitas. a seleção ocorre nas que você consegue convencer a liberar a xoxota, se nenhuma delas te agrada, apenas ignore e continue tentando. ao fazer isso você usa as negras para perder a vergonha e o medinho, e continuar exercitando o seu \"músculo social\" para quando alguma mais bonitinha aparecer.quando eu digo \"valer a pena\", eu estou falando em passar a rola. depósito de porra nenhuma nascida após os anos é material de namoro, o certo é passar o salame e depois deletar o contato, e quem discordar é futuro corno.  Predito:  S\n",
      "Real:  N crl que até pra pessoa transar precisa ter auto-estima pra ter coragem de mostrar seu corpo pra outra pessoa sem se sentir insuficiente, meu deus eu estou destinada a falhar em tudo porque tudo envolve auto-estima e autoconfiança, eu sou uma piada????  Predito:  N\n",
      "Real:  S a conheci no foguinho, eu consegui levá-la para a cama porque ela mesma tomou atitude. após meses apenas ficando e saindo ela chegou disse que estaria com a casa livre e que queria transar, topei na hora, já com medo, é claro.  Predito:  S\n",
      "Real:  S   Predito:  N\n",
      "Real:  S meus caros, vejo vocês falando de tadalafila, qual o preço médio desta porra? vou transar esse fim de semana com uma vagabunda deliciosa e preciso performar bem, porém não sei o preço e preciso ir preparado pra farmácia, já que não tenho dinheiro sobrando pra pagar independentemente do preço.  Predito:  N\n",
      "Predições corretas:  0.828978622327791\n"
     ]
    }
   ],
   "source": [
    "NGRAMS = 2\n",
    "label = {0 : \"N\",\n",
    "         1 : \"S\"}\n",
    "\n",
    "def predict(_text, model, vocab, ngrams):\n",
    "    if len(_text) == 0:\n",
    "        return 0\n",
    "    with torch.no_grad():\n",
    "        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n",
    "        output = model(torch.tensor(_text), torch.tensor([0]))\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "model = model.to('cpu')\n",
    "for entry in test_data[:10]:\n",
    "    print('Real: ', label[int(entry.has_anger[0])], \" \".join(entry.txt), ' Predito: ', label[predict(entry.txt, model, text.vocab, NGRAMS)])\n",
    "\n",
    "#predictions = [predict(entry.txt, model, text.vocab, NGRAMS) for entry in test_data]\n",
    "predictions = [predict(text_pipeline(\"\".join(entry.txt)), model, text.vocab, NGRAMS) == int(entry.has_anger[0]) for entry in test_data]\n",
    "count = 0\n",
    "for value in predictions:\n",
    "    if value == 1:\n",
    "        count += 1\n",
    "print('Predições corretas: ', count / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "6.2-classificao-neural-pretrain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
