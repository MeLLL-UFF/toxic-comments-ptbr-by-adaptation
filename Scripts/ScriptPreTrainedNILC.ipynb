{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torchtext spacy\n",
    "# python3 -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.legacy import data\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l_rvQMf-Jq5g"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, drop_prob, hs1, num_class, freeze=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        if freeze:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(embed_dim, hs1) \n",
    "        self.fc2 = nn.Linear(hs1, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = embedded.view(embedded.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        preds = self.softmax(self.fc2(x))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ew8i1QL2SRVU"
   },
   "outputs": [],
   "source": [
    "### Criando dataset de treino e de teste ###\n",
    "text = data.Field()\n",
    "\n",
    "dataset_number = 1\n",
    "\n",
    "# Lacafe\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech_comb'\n",
    "# train_path = f'./Datasets/Lacafe/df_dataset_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/Lacafe/df_dataset_test_{dataset_number}.csv'\n",
    "\n",
    "# Fortuna\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech_comb'\n",
    "# train_path = f'./Datasets/Fortuna/2019-05-28_portuguese_hate_speech_binary_classification_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/Fortuna/2019-05-28_portuguese_hate_speech_binary_classification_test_{dataset_number}.csv'\n",
    "\n",
    "# Fortuna (undersampling)\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech_comb'\n",
    "# train_path = f'./Datasets/FortunaUndersampling/2019-05-28_portuguese_hate_speech_binary_classification_undersampling_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/FortunaUndersampling/2019-05-28_portuguese_hate_speech_binary_classification_undersampling_test_{dataset_number}.csv'\n",
    "\n",
    "# ToLD-BR\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech'\n",
    "# train_path = f'./Datasets/ToldBR/ToLD-BR-Treated_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/ToldBR/ToLD-BR-Treated_test_{dataset_number}.csv'\n",
    "\n",
    "# ToLD-BR (undersampling)\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech'\n",
    "# train_path = f'./Datasets/ToldBRUndersampling/ToLD-BR-Treated_undersampling_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/ToldBRUndersampling/ToLD-BR-Treated_undersampling_test_{dataset_number}.csv'\n",
    "\n",
    "# ToLD-BR (Updated)\n",
    "text_column = 'text'\n",
    "label_column = 'hatespeech'\n",
    "train_path = f'./Datasets/ToldBRUpdated/ToLD-BR-Treated_train_{dataset_number}.csv'\n",
    "test_path = f'./Datasets/ToldBRUpdated/ToLD-BR-Treated_test_{dataset_number}.csv'\n",
    "\n",
    "# Fortuna (Updated)\n",
    "# text_column = 'text'\n",
    "# label_column = 'hatespeech_comb'\n",
    "# train_path = f'./Datasets/FortunaUpdated/2019-05-28_portuguese_hate_speech_binary_classification_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/FortunaUpdated/2019-05-28_portuguese_hate_speech_binary_classification_test_{dataset_number}.csv'\n",
    "\n",
    "# OffComBr\n",
    "# text_column = 'text'\n",
    "# label_column = 'offensive'\n",
    "# train_path = f'./Datasets/OffComBR/OffComBR2_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/OffComBR/OffComBR2_test_{dataset_number}.csv'\n",
    "\n",
    "# HateBR\n",
    "# text_column = 'instagram_comments'\n",
    "# label_column = 'offensive_language'\n",
    "# train_path = f'./Datasets/HateBR/HateBR_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/HateBR/HateBR_test_{dataset_number}.csv'\n",
    "\n",
    "# MixToldBRLacafe\n",
    "# text_column = 'txt'\n",
    "# label_column = 'offensive'\n",
    "# train_path = f'./Datasets/MixToldBRLacafe/mix_toldbr_lacafe_train_{dataset_number}.csv'\n",
    "# test_path = f'./Datasets/MixToldBRLacafe/mix_toldbr_lacafe_test_{dataset_number}.csv'\n",
    "\n",
    "train_data = data.TabularDataset(path=train_path,\n",
    "                                 format=\"csv\",\n",
    "                                 fields=[\n",
    "                                        ('text', text),\n",
    "                                        ('label', data.Field())\n",
    "                                 ],\n",
    "                                 skip_header=True)\n",
    "\n",
    "test_data = data.TabularDataset(path=test_path,\n",
    "                                format=\"csv\",\n",
    "                                fields=[\n",
    "                                        ('text', text),\n",
    "                                        ('label', data.Field())\n",
    "                                ],\n",
    "                                skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14699"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gz3HmjcvKFEo"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "#PAD_IDX = text.vocab.stoi[text.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SYWhXINHnYrk"
   },
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = 10000\n",
    "# NGRAMS = 2\n",
    "# BATCH_SIZE = 8\n",
    "# EMBED_DIM = 300\n",
    "# NUM_CLASS = 2\n",
    "# DROPOUT_PROB = 0.5\n",
    "# HS1 = 128\n",
    "\n",
    "# model = TextSentiment(VOCAB_SIZE, EMBED_DIM, DROPOUT_PROB, HS1, NUM_CLASS, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jLuNJU-dS0Of"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.8224, -3.7907, -2.7506,  ...,  0.4684,  0.1956,  0.0516],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "\n",
    "pretrained_embeddings = vocab.Vectors(name = 'nilc_embeddings/fasttext_cbow_s300.txt')\n",
    "\n",
    "text.build_vocab(#data,\n",
    "                 train_data, \n",
    "                 test_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = pretrained_embeddings,\n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "VOCAB_SIZE = len(text.vocab.vectors)\n",
    "NGRAMS = 2\n",
    "BATCH_SIZE = 8\n",
    "EMBED_DIM = 300\n",
    "NUM_CLASS = 2\n",
    "DROPOUT_PROB = 0.5\n",
    "HS1 = 128\n",
    "\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, DROPOUT_PROB, HS1, NUM_CLASS, True).to(device)\n",
    "\n",
    "model.embedding.weight.data.copy_(text.vocab.vectors)\n",
    "# UNK_IDX = text.vocab.stoi[text.unk_token]\n",
    " \n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "#model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)\n",
    "\n",
    "#print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-04 23:18:06.998019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-04 23:18:06.998037: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization\n",
    "import spacy\n",
    "import string\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence, lemmatize=False, remove_stop_words=False, remove_punctuations=False):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(sentence)\n",
    "\n",
    "    # Lematizando os tokens e colocando em caixa baixa\n",
    "    if (lemmatize):\n",
    "        mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "    else:\n",
    "        mytokens = [ word.text.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # Removendo stop words\n",
    "    if (remove_stop_words):\n",
    "        mytokens = [ word for word in mytokens if word not in stop_words ]        \n",
    "        \n",
    "    if (remove_punctuations):\n",
    "        mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "        \n",
    "    # Retornando a lista de token reprocessado\n",
    "    return mytokens\n",
    "\n",
    "text_pipeline = lambda x: spacy_tokenizer(x, lemmatize=True, remove_stop_words=True)\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w6tRmuvdO1Ig"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([label_pipeline(entry.label[0]) for entry in batch])\n",
    "    _text = []\n",
    "    for entry in batch:\n",
    "        _entry = []\n",
    "        entry_txt = text_pipeline(\"\".join(entry.text))\n",
    "        for t in entry_txt:\n",
    "            _entry.append(text.vocab.stoi[t])\n",
    "        _text.append(torch.tensor(_entry,dtype=torch.long))\n",
    "    offsets = [0] + [len(entry) for entry in _text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    _text = torch.cat(_text)\n",
    "    #print(' BATCH' , _text)\n",
    "    #print(offsets)\n",
    "    return _text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_HebyntEO56B"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        #output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            #output = model(text)\n",
    "            #print(output, cls)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uendMPTfPBNx",
    "outputId": "3f1ef8f4-8b90-4e7b-a63f-510808c8be36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f49f8be42b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "N_EPOCHS = 1\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "#optimizer = optim.SparseAdam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "# Divide 90% para o treino e 10% para os testes\n",
    "# train_len = int(len(data) * 0.90)\n",
    "# sub_train_, sub_test_ = random_split(data, [train_len, len(data) - train_len])\n",
    "# sub_train_\n",
    "\n",
    "# Do que sobrou do treino, 90% para o treino e 10% para a validacao\n",
    "train_len = int(len(train_data) * 0.90)\n",
    "train_data, valid_data = random_split(train_data, [train_len, len(train_data) - train_len])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5prB___LVkxm",
    "outputId": "239eeca3-5aff-4259-b1f9-9b6c008f8c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, | time in 0.85 minutes and 51 seconds\n",
      "\tLoss: 0.0661(train)\t|\tAcc: 78.4%(train)\n",
      "\tLoss: 0.0007(valid)\t|\tAcc: 80.3%(valid)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_data)\n",
    "    valid_loss, valid_acc = test(valid_data)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1}, | time in {mins} minutes and {secs} seconds')\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlrzrwbBWLtj",
    "outputId": "b25c85cd-5c1d-4429-b432-1f93454b16e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0001(test)\t|\tAcc: 80.6%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_data)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "y31fmkrvF7kD"
   },
   "outputs": [],
   "source": [
    "def ngrams_iterator(token_list, ngrams):\n",
    "    \"\"\"Return an iterator that yields the given tokens and their ngrams.\n",
    "\n",
    "    Arguments:\n",
    "        token_list: A list of tokens\n",
    "        ngrams: the number of ngrams.\n",
    "\n",
    "    Examples:\n",
    "        >>> token_list = ['here', 'we', 'are']\n",
    "        >>> list(ngrams_iterator(token_list, 2))\n",
    "        >>> ['here', 'here we', 'we', 'we are', 'are']\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_ngrams(n):\n",
    "        return zip(*[token_list[i:] for i in range(n)])\n",
    "\n",
    "    for x in token_list:\n",
    "        yield x\n",
    "    for n in range(2, ngrams + 1):\n",
    "        for x in _get_ngrams(n):\n",
    "            yield ' '.join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmtbOGHTFQXh",
    "outputId": "e833cbcf-82e8-44b7-a5a7-92c91f61d275"
   },
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "# label = {0 : \"N\",\n",
    "#          1 : \"S\"}\n",
    "\n",
    "def predict(_text, model, vocab, ngrams):\n",
    "    if len(_text) == 0:\n",
    "        return 0\n",
    "    with torch.no_grad():\n",
    "        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n",
    "        output = model(torch.tensor(_text), torch.tensor([0]))\n",
    "        return output.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_positive': 0, 'true_negative': 5081, 'false_positive': 1, 'false_negative': 1219}\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31420/53302231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'false_positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'false_negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mfscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0macertos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_positive'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'true_negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model = model.to('cpu')\n",
    "\n",
    "# Rodando os testes e obtendo dados da matriz de confusao para calculo das metricas\n",
    "cm = {'true_positive': 0, 'true_negative': 0, 'false_positive': 0, 'false_negative': 0}\n",
    "for entry in test_data:\n",
    "    if (predict(text_pipeline(\"\".join(entry.text)), model, text.vocab, NGRAMS) == int(entry.label[0])):\n",
    "        if int(entry.label[0]) == 1:\n",
    "            cm['true_positive'] += 1\n",
    "        else:\n",
    "            cm['true_negative'] += 1\n",
    "    else:\n",
    "        if int(entry.label[0]) == 0:\n",
    "            cm['false_positive'] += 1\n",
    "        else:\n",
    "            cm['false_negative'] += 1\n",
    "\n",
    "# Metricas\n",
    "print(cm)\n",
    "precision = cm['true_positive'] / (cm['true_positive'] + cm['false_positive'])\n",
    "recall = cm['true_positive'] / (cm['true_positive'] + cm['false_negative'])\n",
    "fscore = 2 * ((precision * recall) / (precision + recall))\n",
    "acertos = cm['true_positive'] + cm['true_negative']\n",
    "total = len(test_data)\n",
    "print(f'Acertos: {acertos}')\n",
    "print(f'Total: {total}')\n",
    "print(f'Accuracy: {str((acertos / total) * 100).replace(\".\", \",\")}')\n",
    "print(f'Precision: {str((precision * 100)).replace(\".\", \",\")}')\n",
    "print(f'Recall: {str((recall * 100)).replace(\".\", \",\")}')\n",
    "print(f'Fscore: {str((fscore * 100)).replace(\".\", \",\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "6.2-classificao-neural-pretrain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
