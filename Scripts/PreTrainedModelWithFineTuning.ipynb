{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d71d5bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn torch datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca33d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"df_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4aef1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "\n",
    "# Embaralha e separa os dados em 10 partes\n",
    "shuffled = data.sample(frac=1)\n",
    "result = np.array_split(shuffled, 20)\n",
    "\n",
    "# Pega 9 partes e reune novamente para formar o conjunto de treino\n",
    "data_frames = []\n",
    "for i in range(19):\n",
    "    data_frames.append(result[i])\n",
    "train_data = reduce(lambda  left,right: pd.merge(left,right,how='outer'), data_frames)\n",
    "\n",
    "# Pega uma parte para formar o conjunto de validacao\n",
    "# val_data = result[18]\n",
    "\n",
    "# Pega a ultima parte para formar o conjunto de teste\n",
    "test_data = result[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "491b299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                docno  has_anger      origin  \\\n",
      "0           3924                27940          1  55chan/pol   \n",
      "1           1916  1141767214728733056          0     twitter   \n",
      "2           2748                59319          1  55chan/pol   \n",
      "3           5516  1141761023944338944          0     twitter   \n",
      "4           4795  1141764131931876992          0     twitter   \n",
      "...          ...                  ...        ...         ...   \n",
      "6884        7469  1141766371686211968          0     twitter   \n",
      "6885        3701                60637          1  55chan/pol   \n",
      "6886        3628                60686          1  55chan/pol   \n",
      "6887        4459  1141783291533677056          0     twitter   \n",
      "6888        3453                62493          1  55chan/pol   \n",
      "\n",
      "                                                    txt  \n",
      "0     eu até pensei em não te responder, mas você é ...  \n",
      "1                                  NOMEPROPRIO original  \n",
      "2     terminei com a única mulher que já me amou nes...  \n",
      "3                                 NOMEPROPRIO contamina  \n",
      "4                    quem é parceiro p tomar um açaí hj  \n",
      "...                                                 ...  \n",
      "6884  filho\\nenquanto eu for tua prioridade, pode vi...  \n",
      "6885  não se passe por mim, desgraçado. rapaz, todo ...  \n",
      "6886  boa noite, anons.então, minha dúvida é quanto ...  \n",
      "6887                                 NOMEPROPRIO pensei  \n",
      "6888  anão, tenho certeza que meu problema é crônico...  \n",
      "\n",
      "[6889 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'docno', 'has_anger', 'origin', 'txt', '__index_level_0__'],\n",
       "        num_rows: 6889\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'docno', 'has_anger', 'origin', 'txt', '__index_level_0__'],\n",
       "        num_rows: 362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "print(train_data)\n",
    "train_data = Dataset.from_pandas(train_data)\n",
    "test_data = Dataset.from_pandas(test_data)\n",
    "\n",
    "raw_datasets = DatasetDict({'train': train_data, 'test': test_data})\n",
    "\n",
    "train_data\n",
    "test_data\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578fe729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-269d05c058f9d497\n",
      "Reusing dataset csv (/home/arthurn/.cache/huggingface/datasets/csv/default-269d05c058f9d497/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacc2697440d4d448d02ffbfb28424e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'docno', 'has_anger', 'origin', 'txt'],\n",
       "        num_rows: 7251\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'docno', 'has_anger', 'origin', 'txt'],\n",
       "        num_rows: 421\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Importação do Dataset\n",
    "\n",
    "# from torch.utils.data.dataset import random_split\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# raw_datasets = load_dataset('csv', data_files={'train': 'df_dataset.csv', 'test':'df_dataset_test.csv'})\n",
    "\n",
    "# raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "888fdb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainTexts Length:  6889\n",
      "TrainLabels Length:  6889\n",
      "TestTexts Length:  362\n",
      "TestLabels Length:  362\n",
      "TestTexts Post Processing Length:  362\n",
      "TestLabels Post Processing Length:  362\n"
     ]
    }
   ],
   "source": [
    "# Tratamento básico dos dados\n",
    "\n",
    "train_texts = raw_datasets['train']['txt']\n",
    "train_labels = raw_datasets['train']['has_anger']\n",
    "test_texts = raw_datasets['test']['txt']\n",
    "test_labels = raw_datasets['test']['has_anger']\n",
    "\n",
    "print(\"TrainTexts Length: \", len(train_texts))\n",
    "print(\"TrainLabels Length: \", len(train_labels))\n",
    "print(\"TestTexts Length: \", len(test_texts))\n",
    "print(\"TestLabels Length: \", len(test_labels))\n",
    "\n",
    "# Removendo elementos None no texto e nas labels\n",
    "elements_none = []\n",
    "for x in range(len(test_texts)):\n",
    "    if (test_texts[x] == None):\n",
    "        elements_none.append(x)\n",
    "\n",
    "for index_none in sorted(elements_none, reverse=True):\n",
    "    test_texts.pop(index_none)\n",
    "    test_labels.pop(index_none)\n",
    "    \n",
    "print(\"TestTexts Post Processing Length: \", len(test_texts))\n",
    "print(\"TestLabels Post Processing Length: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52ea65c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "938292eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainTexts Length:  6613\n",
      "TrainLabels Length:  6613\n",
      "ValidationTexts Length:  276\n",
      "ValidationLabels Length:  276\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainTexts Length: \", len(train_texts))\n",
    "print(\"TrainLabels Length: \", len(train_labels))\n",
    "print(\"ValidationTexts Length: \", len(val_texts))\n",
    "print(\"ValidationLabels Length: \", len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8521c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o Tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac10da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenização dos datasets\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2b5737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voltando os datasets tokenizados para instâncias da classe de Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6575c9e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(val_dataset.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7741a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine Tuning do Modelo\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=2)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.sum().backward() # loss.backward() # Tive que alterar, pois loss.backward() é, implicitamente, loss.backward(torch.Tensor([1]))\n",
    "        # e no caso, deveria ser um vetor com mais de um elemento\n",
    "        # https://discuss.pytorch.org/t/loss-backward-raises-error-grad-can-be-implicitly-created-only-for-scalar-outputs/12152\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fc8fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Salvando o modelo Pre treinado\n",
    "\n",
    "# model.save_pretrained(\"pretrained-model-bert-base-portuguese-cased-fine-tuning-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2411c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 6958 6959 6960]\n",
      "[   4    5   25   31   83   87  113  117  122  133  163  170  189  190\n",
      "  194  196  226  228  234  255  256  262  265  268  271  273  279  287\n",
      "  292  323  332  333  354  362  366  397  402  407  426  451  454  485\n",
      "  501  525  532  556  558  571  577  585  598  601  605  616  622  623\n",
      "  630  638  643  653  657  662  689  693  702  704  709  710  717  718\n",
      "  719  722  727  733  737  744  745  757  769  789  807  811  817  829\n",
      "  830  852  853  858  860  870  873  891  893  899  902  908  930  945\n",
      "  952  958  962  965  969  974 1013 1014 1049 1057 1058 1069 1070 1088\n",
      " 1089 1132 1140 1141 1142 1145 1155 1157 1185 1194 1206 1207 1221 1250\n",
      " 1266 1268 1274 1278 1286 1303 1312 1314 1325 1328 1348 1351 1354 1366\n",
      " 1385 1388 1389 1406 1414 1424 1429 1473 1480 1484 1485 1493 1504 1506\n",
      " 1508 1509 1514 1520 1527 1533 1551 1556 1560 1576 1578 1583 1586 1591\n",
      " 1592 1645 1652 1655 1665 1682 1690 1691 1703 1722 1724 1727 1733 1741\n",
      " 1742 1750 1779 1781 1836 1872 1875 1877 1884 1907 1917 1920 1940 1944\n",
      " 1947 1997 2026 2029 2034 2045 2047 2051 2059 2083 2099 2129 2135 2158\n",
      " 2159 2174 2176 2178 2183 2185 2214 2220 2222 2227 2230 2234 2236 2237\n",
      " 2243 2264 2283 2314 2323 2332 2337 2349 2356 2357 2377 2387 2404 2405\n",
      " 2406 2407 2409 2412 2417 2425 2427 2433 2442 2447 2489 2495 2502 2533\n",
      " 2537 2540 2543 2553 2559 2561 2588 2590 2591 2611 2631 2640 2642 2646\n",
      " 2673 2688 2702 2703 2719 2730 2731 2751 2760 2763 2766 2783 2791 2803\n",
      " 2806 2807 2815 2828 2831 2847 2856 2859 2862 2866 2876 2880 2891 2900\n",
      " 2906 2911 2923 2927 2929 2943 2954 2960 2979 2981 2991 2994 2998 3009\n",
      " 3018 3031 3039 3042 3045 3061 3081 3092 3096 3100 3109 3112 3117 3119\n",
      " 3120 3130 3138 3155 3160 3165 3173 3195 3199 3214 3218 3238 3245 3248\n",
      " 3251 3254 3255 3298 3313 3317 3319 3335 3349 3350 3352 3391 3394 3403\n",
      " 3405 3407 3425 3427 3450 3461 3476 3479 3494 3502 3509 3512 3515 3521\n",
      " 3527 3531 3539 3542 3557 3601 3607 3610 3616 3620 3633 3643 3656 3665\n",
      " 3674 3682 3684 3703 3706 3709 3713 3723 3765 3774 3794 3825 3843 3870\n",
      " 3872 3905 3906 3913 3923 3930 3946 3948 3955 3962 3974 3980 3986 4025\n",
      " 4027 4036 4038 4055 4067 4074 4089 4091 4128 4129 4134 4136 4145 4147\n",
      " 4151 4152 4159 4170 4173 4188 4191 4203 4210 4216 4217 4224 4225 4261\n",
      " 4265 4278 4288 4289 4299 4300 4341 4344 4377 4378 4379 4380 4394 4400\n",
      " 4410 4414 4432 4433 4447 4502 4510 4516 4521 4534 4536 4537 4538 4550\n",
      " 4559 4563 4567 4572 4581 4585 4603 4605 4614 4634 4658 4665 4687 4704\n",
      " 4705 4708 4720 4742 4747 4750 4759 4761 4768 4772 4789 4801 4820 4844\n",
      " 4851 4891 4910 4911 4916 4921 4934 4938 4961 4988 5006 5012 5029 5035\n",
      " 5036 5039 5046 5048 5069 5077 5084 5091 5099 5125 5133 5136 5159 5167\n",
      " 5175 5197 5203 5241 5247 5252 5260 5272 5274 5288 5302 5324 5336 5349\n",
      " 5358 5361 5371 5389 5392 5399 5409 5426 5449 5459 5466 5476 5478 5481\n",
      " 5484 5486 5487 5495 5514 5530 5533 5559 5567 5583 5589 5590 5594 5603\n",
      " 5638 5640 5648 5661 5666 5684 5686 5697 5699 5701 5702 5715 5723 5725\n",
      " 5732 5734 5740 5751 5757 5779 5780 5781 5795 5843 5844 5853 5855 5857\n",
      " 5859 5887 5905 5963 5969 5989 5991 5992 5995 6017 6020 6032 6037 6046\n",
      " 6047 6049 6055 6056 6067 6068 6075 6087 6088 6095 6100 6101 6121 6142\n",
      " 6144 6148 6151 6152 6156 6166 6168 6177 6180 6197 6201 6216 6235 6242\n",
      " 6245 6262 6264 6266 6294 6295 6308 6320 6326 6327 6332 6356 6361 6365\n",
      " 6374 6402 6405 6419 6422 6426 6429 6430 6436 6450 6461 6485 6487 6500\n",
      " 6501 6508 6522 6542 6554 6558 6583 6589 6593 6607 6634 6649 6654 6662\n",
      " 6673 6691 6704 6718 6720 6725 6764 6777 6788 6791 6792 6802 6817 6840\n",
      " 6846 6853 6855 6864 6898 6900 6903 6907 6908 6918 6935]\n",
      "\n",
      "[   1    2    3 ... 6958 6959 6960]\n",
      "[   0    6   15   27   43   62   65   70   72   73   74   98  100  121\n",
      "  136  149  155  156  167  174  202  220  232  236  254  259  277  283\n",
      "  293  297  310  312  320  322  324  340  347  352  359  367  384  400\n",
      "  419  439  441  446  448  459  494  502  510  516  519  563  566  573\n",
      "  584  599  606  608  626  635  641  644  645  646  695  720  723  731\n",
      "  739  749  751  752  754  759  762  764  768  777  778  788  793  795\n",
      "  796  799  800  812  836  839  863  881  884  885  894  896  901  912\n",
      "  913  925  934  941  948  954  968  977  980  999 1017 1029 1048 1076\n",
      " 1079 1082 1091 1094 1114 1121 1125 1126 1131 1149 1167 1170 1182 1198\n",
      " 1199 1200 1214 1223 1225 1245 1251 1253 1254 1292 1305 1319 1332 1341\n",
      " 1362 1374 1377 1378 1394 1401 1411 1415 1422 1428 1450 1461 1463 1479\n",
      " 1486 1487 1489 1490 1503 1518 1528 1543 1553 1569 1574 1587 1620 1621\n",
      " 1632 1635 1636 1667 1686 1687 1695 1708 1714 1717 1723 1726 1753 1757\n",
      " 1769 1772 1791 1794 1796 1800 1815 1819 1826 1852 1869 1891 1895 1901\n",
      " 1902 1903 1905 1906 1910 1925 1929 1931 1937 1946 1963 1973 1977 1983\n",
      " 1987 2000 2018 2042 2056 2061 2065 2067 2070 2073 2088 2102 2116 2131\n",
      " 2136 2144 2148 2154 2160 2161 2170 2179 2188 2202 2203 2217 2229 2239\n",
      " 2270 2277 2287 2291 2302 2307 2319 2344 2362 2381 2396 2399 2413 2424\n",
      " 2435 2444 2445 2448 2461 2470 2472 2481 2499 2504 2518 2519 2522 2541\n",
      " 2554 2575 2589 2592 2608 2609 2612 2620 2624 2628 2664 2693 2694 2695\n",
      " 2705 2716 2725 2729 2738 2753 2768 2776 2781 2785 2795 2796 2799 2801\n",
      " 2811 2832 2833 2867 2873 2893 2904 2908 2920 2934 2946 2952 2955 2964\n",
      " 2965 2966 2972 2974 2980 2989 2993 2995 2997 3010 3014 3024 3028 3036\n",
      " 3041 3059 3063 3074 3084 3106 3123 3128 3175 3177 3186 3192 3201 3220\n",
      " 3231 3232 3233 3234 3250 3256 3263 3270 3273 3275 3276 3301 3303 3306\n",
      " 3327 3334 3339 3346 3357 3360 3363 3375 3381 3400 3401 3414 3426 3433\n",
      " 3441 3449 3451 3464 3470 3471 3475 3478 3483 3508 3529 3532 3547 3552\n",
      " 3566 3586 3604 3605 3611 3628 3630 3638 3641 3642 3672 3676 3695 3701\n",
      " 3726 3748 3755 3760 3773 3780 3809 3813 3827 3851 3868 3878 3885 3886\n",
      " 3887 3910 3911 3920 3927 3929 3940 3942 3944 3951 3954 3956 3957 3959\n",
      " 3971 3987 3994 4014 4017 4022 4034 4039 4042 4043 4051 4059 4060 4066\n",
      " 4070 4078 4084 4085 4095 4097 4103 4119 4122 4125 4166 4182 4186 4193\n",
      " 4194 4201 4204 4222 4227 4251 4264 4273 4274 4282 4283 4292 4306 4310\n",
      " 4325 4337 4372 4393 4399 4405 4406 4431 4448 4452 4456 4457 4460 4464\n",
      " 4466 4487 4514 4515 4519 4520 4528 4531 4554 4564 4577 4578 4594 4615\n",
      " 4630 4633 4636 4671 4685 4713 4714 4717 4725 4733 4738 4743 4745 4746\n",
      " 4767 4774 4800 4824 4873 4875 4876 4883 4895 4896 4899 4918 4919 4931\n",
      " 4939 4944 4949 4953 4973 4976 4982 4983 4991 4998 5003 5025 5034 5037\n",
      " 5038 5042 5060 5064 5068 5071 5075 5083 5092 5094 5127 5138 5151 5156\n",
      " 5157 5172 5173 5206 5219 5258 5261 5269 5278 5286 5296 5299 5307 5318\n",
      " 5321 5327 5329 5355 5377 5402 5410 5412 5416 5420 5431 5435 5445 5462\n",
      " 5463 5471 5490 5526 5529 5534 5544 5550 5552 5561 5571 5581 5588 5651\n",
      " 5653 5659 5675 5693 5713 5736 5760 5765 5770 5788 5804 5814 5819 5834\n",
      " 5846 5872 5890 5892 5895 5899 5916 5918 5944 5985 6001 6002 6005 6015\n",
      " 6031 6042 6051 6062 6066 6078 6082 6098 6108 6127 6147 6204 6257 6260\n",
      " 6269 6283 6314 6316 6333 6338 6341 6353 6359 6409 6410 6411 6428 6438\n",
      " 6440 6449 6451 6458 6470 6480 6486 6493 6510 6528 6536 6556 6571 6572\n",
      " 6573 6579 6584 6586 6591 6596 6600 6605 6621 6627 6628 6631 6645 6656\n",
      " 6661 6666 6696 6714 6716 6758 6766 6770 6772 6783 6785 6816 6821 6822\n",
      " 6842 6849 6852 6880 6882 6883 6891 6921 6926 6947]\n",
      "\n",
      "[   0    1    2 ... 6958 6959 6960]\n",
      "[  12   32   45   47   50   52   53   69   77   85  104  105  115  130\n",
      "  179  180  198  199  225  233  245  250  251  253  258  264  275  291\n",
      "  305  316  318  321  346  376  382  416  424  425  433  438  443  463\n",
      "  470  481  482  489  491  504  506  513  543  547  548  553  555  561\n",
      "  575  580  617  621  627  636  661  663  665  677  690  700  713  716\n",
      "  767  782  808  816  832  846  898  907  911  921  922  923  932  938\n",
      "  944  947  957  964  976  986  990 1007 1018 1022 1026 1033 1036 1042\n",
      " 1053 1068 1077 1095 1096 1099 1100 1105 1108 1111 1122 1133 1143 1158\n",
      " 1168 1172 1184 1187 1201 1202 1210 1228 1236 1252 1256 1257 1262 1273\n",
      " 1276 1279 1289 1309 1339 1345 1391 1397 1402 1403 1423 1425 1426 1453\n",
      " 1455 1470 1474 1481 1497 1501 1516 1526 1536 1547 1557 1559 1579 1585\n",
      " 1599 1615 1624 1631 1648 1659 1662 1675 1678 1679 1689 1698 1701 1702\n",
      " 1711 1720 1728 1737 1743 1745 1756 1758 1761 1774 1778 1806 1818 1831\n",
      " 1833 1838 1840 1858 1868 1890 1912 1919 1930 1948 1956 1982 1988 1990\n",
      " 1992 2001 2007 2010 2021 2035 2066 2080 2108 2120 2127 2143 2151 2165\n",
      " 2167 2186 2190 2196 2198 2205 2207 2211 2218 2221 2225 2262 2272 2299\n",
      " 2301 2317 2326 2334 2340 2345 2348 2358 2369 2374 2414 2418 2440 2441\n",
      " 2443 2450 2457 2462 2469 2471 2514 2548 2550 2558 2566 2571 2583 2593\n",
      " 2595 2596 2605 2614 2617 2621 2625 2632 2637 2644 2655 2657 2663 2676\n",
      " 2677 2684 2686 2689 2715 2734 2735 2746 2747 2750 2752 2757 2767 2778\n",
      " 2804 2818 2824 2840 2848 2855 2903 2932 2936 2945 2956 2957 2963 2967\n",
      " 2977 2999 3013 3016 3021 3075 3082 3086 3113 3121 3135 3139 3143 3145\n",
      " 3151 3159 3174 3240 3247 3261 3272 3280 3305 3307 3311 3322 3340 3362\n",
      " 3372 3378 3379 3380 3393 3419 3422 3424 3435 3452 3485 3491 3493 3511\n",
      " 3514 3517 3520 3546 3551 3559 3567 3571 3572 3580 3581 3584 3585 3596\n",
      " 3626 3637 3640 3653 3691 3692 3698 3704 3715 3724 3727 3734 3740 3742\n",
      " 3746 3756 3759 3761 3764 3768 3779 3782 3787 3801 3802 3812 3836 3856\n",
      " 3859 3864 3865 3871 3876 3882 3892 3901 3917 3924 3939 3941 3972 3975\n",
      " 3985 3993 4000 4010 4012 4021 4046 4057 4063 4068 4075 4079 4094 4096\n",
      " 4098 4099 4123 4127 4132 4137 4149 4169 4179 4187 4190 4192 4205 4211\n",
      " 4226 4231 4245 4248 4262 4263 4279 4287 4298 4314 4350 4359 4373 4386\n",
      " 4389 4407 4423 4426 4438 4458 4461 4463 4471 4474 4482 4484 4495 4518\n",
      " 4525 4527 4532 4543 4548 4553 4565 4571 4575 4583 4601 4644 4649 4651\n",
      " 4662 4663 4672 4679 4690 4692 4695 4719 4754 4762 4787 4803 4816 4817\n",
      " 4827 4845 4848 4857 4870 4871 4877 4881 4893 4898 4908 4912 4913 4915\n",
      " 4924 4928 4932 4947 4950 4955 4962 4970 4990 4993 4994 5017 5032 5047\n",
      " 5067 5085 5114 5118 5149 5168 5190 5201 5212 5220 5240 5243 5253 5257\n",
      " 5267 5284 5293 5308 5311 5312 5326 5330 5350 5356 5360 5365 5368 5380\n",
      " 5414 5418 5424 5428 5429 5439 5450 5469 5482 5492 5501 5510 5513 5518\n",
      " 5528 5536 5543 5558 5566 5573 5577 5580 5587 5593 5602 5606 5611 5614\n",
      " 5620 5625 5630 5631 5642 5644 5700 5703 5708 5710 5729 5730 5741 5743\n",
      " 5747 5752 5763 5773 5782 5784 5805 5808 5811 5812 5816 5818 5821 5823\n",
      " 5827 5838 5845 5849 5860 5861 5867 5869 5881 5908 5913 5919 5934 5982\n",
      " 5988 5996 6003 6004 6035 6040 6050 6054 6063 6074 6080 6094 6114 6133\n",
      " 6136 6139 6141 6172 6183 6188 6198 6207 6210 6212 6230 6233 6236 6237\n",
      " 6251 6259 6265 6288 6305 6312 6328 6347 6348 6357 6363 6370 6372 6376\n",
      " 6386 6406 6415 6425 6427 6431 6447 6453 6457 6459 6469 6496 6503 6511\n",
      " 6574 6582 6597 6611 6643 6648 6669 6687 6706 6734 6736 6742 6747 6754\n",
      " 6762 6763 6776 6778 6807 6808 6809 6813 6830 6833 6834 6835 6861 6865\n",
      " 6870 6890 6895 6896 6897 6913 6919 6931 6953 6954]\n",
      "\n",
      "[   0    2    3 ... 6957 6958 6959]\n",
      "[   1   14   28   37   39   41   54   55   60   67   75   81   89   97\n",
      "  107  131  140  141  153  158  161  164  175  178  205  214  227  235\n",
      "  238  249  272  299  308  309  315  326  334  344  345  364  374  387\n",
      "  392  427  437  444  445  450  455  460  483  488  498  499  500  505\n",
      "  507  508  537  539  541  545  557  569  582  592  604  613  619  620\n",
      "  642  655  681  692  694  696  721  728  730  742  758  771  773  780\n",
      "  819  869  874  876  879  883  887  926  931  967  971 1008 1010 1020\n",
      " 1021 1031 1034 1037 1067 1071 1081 1087 1090 1102 1116 1117 1124 1127\n",
      " 1135 1137 1139 1161 1162 1189 1237 1240 1255 1285 1287 1298 1301 1320\n",
      " 1330 1337 1342 1344 1347 1352 1359 1361 1373 1384 1386 1404 1416 1420\n",
      " 1441 1444 1458 1510 1524 1525 1529 1531 1539 1540 1552 1563 1584 1590\n",
      " 1610 1619 1627 1629 1637 1638 1639 1646 1647 1660 1668 1700 1713 1718\n",
      " 1765 1776 1787 1788 1801 1803 1805 1808 1809 1812 1832 1834 1855 1860\n",
      " 1862 1876 1880 1881 1908 1928 1935 1951 1957 1959 2008 2037 2043 2046\n",
      " 2048 2062 2064 2081 2089 2091 2093 2097 2098 2118 2125 2126 2209 2216\n",
      " 2226 2232 2235 2238 2240 2245 2247 2256 2266 2276 2288 2338 2351 2371\n",
      " 2382 2383 2388 2402 2411 2420 2455 2477 2516 2529 2531 2532 2534 2536\n",
      " 2565 2594 2601 2602 2610 2615 2643 2654 2659 2674 2675 2692 2696 2697\n",
      " 2708 2720 2743 2744 2745 2754 2755 2762 2764 2784 2787 2788 2790 2793\n",
      " 2820 2821 2825 2826 2838 2839 2841 2852 2858 2860 2872 2874 2875 2888\n",
      " 2895 2913 2914 2918 2931 2962 2976 2983 3001 3003 3004 3005 3019 3054\n",
      " 3076 3099 3102 3116 3126 3147 3148 3170 3172 3183 3184 3187 3193 3194\n",
      " 3203 3211 3217 3219 3221 3226 3235 3243 3244 3249 3257 3258 3266 3271\n",
      " 3277 3282 3309 3310 3323 3328 3347 3358 3373 3374 3376 3389 3390 3396\n",
      " 3397 3398 3402 3412 3413 3416 3430 3432 3443 3447 3454 3457 3459 3463\n",
      " 3467 3473 3477 3490 3492 3501 3516 3522 3538 3553 3591 3598 3602 3606\n",
      " 3614 3619 3625 3631 3635 3647 3654 3657 3660 3688 3699 3710 3717 3721\n",
      " 3739 3747 3753 3758 3771 3775 3803 3804 3807 3808 3818 3842 3844 3849\n",
      " 3855 3858 3883 3898 3909 3914 3915 3938 3947 3950 3999 4015 4029 4030\n",
      " 4052 4077 4106 4108 4111 4115 4140 4142 4181 4189 4214 4215 4220 4232\n",
      " 4234 4281 4303 4307 4313 4320 4332 4333 4334 4336 4343 4371 4381 4383\n",
      " 4384 4392 4398 4402 4408 4409 4417 4437 4445 4476 4493 4517 4526 4542\n",
      " 4546 4561 4580 4582 4613 4616 4655 4683 4688 4716 4721 4730 4755 4777\n",
      " 4788 4790 4792 4806 4821 4841 4846 4859 4864 4866 4872 4882 4904 4909\n",
      " 4929 4930 4937 4941 4959 4966 4968 4971 4972 4974 4978 4995 5001 5015\n",
      " 5023 5030 5040 5041 5051 5053 5055 5063 5072 5076 5080 5081 5088 5106\n",
      " 5110 5113 5116 5135 5137 5139 5153 5176 5181 5191 5202 5208 5216 5222\n",
      " 5223 5226 5234 5235 5271 5275 5280 5289 5294 5297 5310 5328 5332 5346\n",
      " 5363 5373 5379 5383 5391 5407 5415 5434 5438 5447 5448 5452 5454 5470\n",
      " 5488 5523 5524 5538 5553 5555 5562 5564 5576 5591 5599 5615 5627 5629\n",
      " 5652 5654 5658 5662 5663 5672 5704 5707 5719 5724 5767 5789 5796 5797\n",
      " 5798 5799 5817 5825 5831 5837 5841 5870 5874 5877 5885 5900 5904 5915\n",
      " 5926 5930 5932 5958 5966 5971 5974 5976 5983 5984 6012 6027 6028 6030\n",
      " 6048 6070 6071 6072 6076 6089 6118 6119 6128 6138 6161 6163 6169 6174\n",
      " 6178 6181 6190 6196 6215 6224 6240 6250 6258 6275 6284 6286 6287 6292\n",
      " 6302 6340 6342 6345 6351 6362 6388 6394 6421 6423 6464 6471 6492 6494\n",
      " 6498 6524 6525 6533 6540 6548 6551 6557 6580 6601 6604 6606 6617 6630\n",
      " 6638 6644 6653 6676 6677 6678 6679 6681 6689 6694 6710 6723 6729 6738\n",
      " 6767 6771 6779 6784 6787 6793 6796 6801 6804 6823 6825 6848 6863 6873\n",
      " 6874 6878 6879 6888 6889 6909 6950 6952 6955 6960]\n",
      "\n",
      "[   0    1    2 ... 6957 6958 6960]\n",
      "[  22   71   88   96  110  134  135  143  154  177  183  185  192  213\n",
      "  216  243  263  269  276  281  288  319  348  369  381  385  391  399\n",
      "  412  413  430  440  453  456  474  476  480  486  534  535  536  540\n",
      "  560  562  570  574  579  593  595  603  637  640  648  650  654  660\n",
      "  684  705  770  776  784  792  794  797  813  818  822  841  844  850\n",
      "  862  872  875  895  905  906  916  943  949  960  972  991  993 1002\n",
      " 1003 1019 1025 1030 1040 1044 1045 1054 1055 1097 1109 1123 1129 1130\n",
      " 1159 1174 1179 1188 1193 1203 1209 1213 1229 1230 1231 1234 1244 1246\n",
      " 1261 1263 1264 1270 1271 1281 1282 1308 1310 1323 1324 1326 1327 1338\n",
      " 1343 1355 1356 1360 1364 1382 1390 1396 1407 1409 1412 1446 1449 1464\n",
      " 1483 1505 1521 1542 1545 1562 1570 1582 1589 1603 1613 1643 1644 1651\n",
      " 1653 1654 1663 1664 1671 1672 1676 1680 1693 1721 1735 1738 1759 1762\n",
      " 1775 1783 1790 1817 1823 1825 1835 1839 1844 1845 1847 1849 1850 1861\n",
      " 1867 1878 1885 1888 1894 1898 1916 1918 1921 1922 1945 1976 1979 1985\n",
      " 2005 2006 2013 2027 2032 2041 2050 2054 2075 2104 2106 2111 2124 2130\n",
      " 2139 2153 2157 2169 2213 2231 2241 2258 2268 2269 2275 2280 2282 2286\n",
      " 2290 2295 2304 2336 2341 2346 2352 2359 2364 2370 2384 2386 2391 2400\n",
      " 2403 2410 2434 2438 2449 2467 2473 2479 2483 2484 2485 2490 2496 2497\n",
      " 2501 2511 2521 2525 2535 2556 2564 2572 2573 2597 2603 2607 2641 2647\n",
      " 2652 2656 2665 2668 2681 2687 2707 2714 2758 2765 2770 2779 2794 2797\n",
      " 2798 2800 2808 2810 2816 2827 2830 2837 2846 2869 2877 2886 2889 2890\n",
      " 2899 2902 2907 2912 2921 2928 2930 2940 2942 2990 2992 3002 3008 3011\n",
      " 3017 3029 3034 3035 3040 3050 3052 3053 3056 3064 3071 3072 3080 3129\n",
      " 3142 3158 3188 3204 3207 3208 3213 3223 3229 3269 3274 3297 3321 3332\n",
      " 3338 3342 3344 3351 3353 3364 3367 3368 3370 3409 3415 3428 3431 3438\n",
      " 3444 3456 3466 3489 3505 3513 3536 3543 3558 3575 3576 3583 3608 3615\n",
      " 3621 3622 3648 3651 3661 3666 3669 3671 3678 3690 3693 3700 3719 3722\n",
      " 3731 3741 3763 3767 3791 3811 3822 3829 3830 3835 3837 3857 3860 3862\n",
      " 3866 3875 3877 3894 3899 3907 3908 3912 3926 3945 3960 3964 3967 3968\n",
      " 3984 3991 4002 4024 4032 4035 4040 4054 4062 4083 4088 4093 4107 4114\n",
      " 4120 4124 4146 4163 4172 4174 4180 4184 4198 4200 4202 4206 4209 4212\n",
      " 4286 4294 4302 4321 4327 4342 4352 4368 4375 4395 4396 4411 4420 4422\n",
      " 4424 4425 4436 4444 4449 4479 4490 4492 4494 4499 4500 4504 4505 4509\n",
      " 4511 4541 4544 4551 4552 4557 4566 4589 4599 4606 4620 4623 4627 4635\n",
      " 4639 4646 4669 4675 4676 4693 4698 4710 4724 4734 4735 4751 4770 4804\n",
      " 4805 4809 4811 4812 4813 4819 4828 4829 4830 4831 4834 4839 4850 4860\n",
      " 4862 4878 4897 4906 4907 4936 4969 4981 5000 5010 5054 5061 5087 5093\n",
      " 5096 5102 5104 5111 5120 5130 5155 5164 5169 5170 5171 5186 5207 5210\n",
      " 5221 5233 5245 5254 5255 5265 5273 5290 5291 5295 5301 5305 5306 5313\n",
      " 5319 5320 5335 5338 5340 5366 5374 5384 5388 5401 5405 5408 5430 5446\n",
      " 5453 5455 5474 5477 5485 5489 5494 5503 5508 5509 5525 5537 5554 5578\n",
      " 5609 5610 5616 5617 5619 5622 5628 5643 5667 5670 5671 5674 5688 5691\n",
      " 5728 5738 5758 5764 5768 5802 5809 5815 5828 5848 5863 5873 5882 5883\n",
      " 5901 5902 5906 5909 5929 5956 5957 5964 5973 5990 5998 5999 6007 6010\n",
      " 6011 6014 6019 6034 6057 6073 6099 6112 6122 6126 6129 6131 6143 6153\n",
      " 6154 6184 6205 6208 6218 6220 6223 6227 6241 6247 6248 6254 6281 6289\n",
      " 6299 6303 6307 6323 6354 6355 6382 6384 6389 6417 6420 6433 6444 6455\n",
      " 6473 6477 6483 6491 6505 6515 6516 6530 6547 6568 6577 6592 6602 6616\n",
      " 6622 6635 6641 6667 6698 6731 6735 6774 6789 6794 6827 6829 6837 6838\n",
      " 6850 6885 6906 6924 6929 6933 6943 6946 6956 6959]\n",
      "\n",
      "[   0    1    2 ... 6958 6959 6960]\n",
      "[  11   16   26   29   30   38   44   58   64  118  124  132  138  146\n",
      "  176  187  188  191  200  206  212  224  230  237  239  242  270  286\n",
      "  294  303  306  307  314  328  331  337  339  341  349  371  386  403\n",
      "  405  409  410  415  421  435  442  449  462  467  472  492  495  521\n",
      "  524  551  564  565  567  568  609  615  628  649  669  676  679  682\n",
      "  686  736  738  756  765  774  804  821  835  849  867  886  897  903\n",
      "  909  914  919  940  978  988  995  996 1006 1011 1012 1015 1032 1043\n",
      " 1046 1051 1060 1061 1065 1073 1086 1101 1112 1119 1128 1163 1175 1180\n",
      " 1181 1183 1191 1196 1204 1205 1218 1222 1226 1232 1239 1260 1272 1283\n",
      " 1288 1302 1336 1350 1368 1372 1376 1383 1400 1448 1459 1460 1471 1482\n",
      " 1519 1532 1548 1549 1554 1568 1604 1605 1608 1611 1628 1658 1661 1677\n",
      " 1725 1739 1749 1754 1755 1763 1767 1789 1807 1811 1821 1822 1827 1828\n",
      " 1829 1854 1886 1887 1900 1914 1923 1936 1938 1952 1962 1964 1966 1991\n",
      " 1994 1996 2015 2028 2036 2038 2052 2057 2063 2068 2069 2074 2078 2079\n",
      " 2087 2090 2109 2110 2115 2121 2133 2138 2146 2171 2173 2177 2184 2191\n",
      " 2194 2223 2233 2248 2251 2255 2263 2279 2281 2284 2294 2297 2298 2308\n",
      " 2312 2313 2318 2342 2343 2347 2361 2367 2368 2380 2393 2394 2428 2429\n",
      " 2436 2458 2466 2498 2505 2523 2524 2528 2551 2562 2570 2576 2604 2626\n",
      " 2635 2653 2666 2672 2679 2690 2709 2726 2737 2741 2789 2792 2802 2836\n",
      " 2842 2851 2864 2865 2878 2881 2898 2905 2919 2924 2935 2937 2938 2969\n",
      " 2975 3015 3020 3032 3046 3049 3065 3070 3073 3085 3088 3114 3122 3131\n",
      " 3133 3137 3140 3153 3161 3167 3171 3189 3222 3224 3227 3246 3252 3262\n",
      " 3267 3268 3281 3283 3291 3316 3320 3341 3387 3399 3417 3418 3439 3453\n",
      " 3460 3462 3465 3510 3518 3519 3523 3530 3533 3545 3555 3560 3565 3570\n",
      " 3582 3595 3627 3636 3649 3658 3663 3670 3686 3696 3705 3708 3720 3745\n",
      " 3776 3777 3792 3793 3795 3806 3810 3816 3823 3839 3840 3854 3861 3889\n",
      " 3891 3897 3900 3904 3916 3919 3933 3943 3952 3958 3963 3966 3988 3990\n",
      " 3996 3997 4001 4018 4037 4044 4045 4053 4056 4064 4069 4072 4076 4080\n",
      " 4100 4109 4118 4121 4139 4154 4155 4164 4167 4168 4178 4185 4195 4241\n",
      " 4247 4250 4254 4255 4266 4280 4290 4304 4316 4318 4324 4331 4338 4351\n",
      " 4362 4365 4388 4401 4403 4415 4427 4429 4440 4441 4488 4496 4507 4523\n",
      " 4535 4556 4587 4595 4596 4611 4624 4626 4629 4631 4645 4647 4656 4657\n",
      " 4673 4680 4684 4689 4696 4699 4700 4703 4709 4727 4749 4765 4766 4769\n",
      " 4776 4782 4785 4786 4814 4832 4836 4837 4847 4849 4867 4868 4874 4879\n",
      " 4884 4885 4917 4925 4946 4963 4964 4967 4984 4999 5007 5019 5021 5022\n",
      " 5056 5066 5070 5095 5100 5112 5115 5117 5122 5126 5129 5134 5145 5148\n",
      " 5163 5182 5199 5200 5213 5215 5218 5229 5238 5249 5279 5281 5285 5317\n",
      " 5325 5333 5334 5337 5347 5353 5385 5394 5395 5406 5417 5419 5432 5440\n",
      " 5444 5464 5479 5483 5493 5499 5505 5507 5516 5531 5532 5541 5542 5548\n",
      " 5551 5569 5579 5582 5584 5600 5626 5632 5656 5665 5680 5689 5690 5709\n",
      " 5718 5722 5735 5745 5746 5750 5759 5774 5775 5776 5792 5793 5800 5822\n",
      " 5835 5839 5842 5850 5852 5868 5875 5880 5889 5891 5903 5924 5933 5936\n",
      " 5940 5942 5952 5960 5965 5968 5972 5975 5979 6013 6021 6033 6052 6059\n",
      " 6083 6084 6110 6113 6117 6123 6134 6137 6146 6157 6158 6159 6160 6173\n",
      " 6185 6192 6203 6209 6217 6222 6228 6231 6234 6239 6246 6255 6291 6306\n",
      " 6318 6324 6329 6331 6358 6364 6368 6369 6381 6383 6400 6412 6413 6418\n",
      " 6443 6448 6454 6462 6463 6465 6475 6484 6502 6506 6523 6529 6537 6544\n",
      " 6549 6555 6564 6567 6587 6615 6623 6639 6642 6646 6650 6652 6658 6682\n",
      " 6700 6708 6717 6732 6733 6750 6759 6775 6790 6806 6812 6815 6819 6843\n",
      " 6859 6869 6875 6905 6912 6914 6940 6945 6951 6957]\n",
      "\n",
      "[   0    1    4 ... 6958 6959 6960]\n",
      "[   2    3    7    8   13   17   23   34   40   56   76   80   84   91\n",
      "   92   94  101  102  111  123  126  129  137  142  151  165  168  182\n",
      "  201  208  211  229  231  240  246  260  266  267  274  300  327  338\n",
      "  350  358  363  365  375  377  380  396  398  406  428  429  431  436\n",
      "  452  465  469  484  527  528  530  531  538  550  552  572  578  589\n",
      "  596  597  600  610  624  629  631  651  664  683  688  697  701  703\n",
      "  708  711  715  729  734  735  741  746  750  753  760  761  772  779\n",
      "  791  803  810  814  823  826  838  842  856  857  866  878  882  904\n",
      "  937  961  975  982  983 1047 1052 1064 1074 1083 1093 1104 1107 1115\n",
      " 1136 1148 1150 1153 1212 1215 1219 1227 1233 1242 1248 1265 1275 1291\n",
      " 1296 1300 1306 1311 1313 1315 1331 1333 1349 1367 1381 1399 1413 1421\n",
      " 1427 1431 1439 1447 1451 1452 1457 1467 1472 1475 1492 1494 1496 1498\n",
      " 1513 1515 1534 1546 1558 1564 1565 1573 1575 1577 1580 1594 1597 1600\n",
      " 1633 1640 1641 1656 1669 1670 1674 1681 1685 1696 1704 1705 1707 1709\n",
      " 1729 1751 1752 1764 1768 1777 1795 1798 1799 1841 1851 1853 1857 1859\n",
      " 1864 1871 1883 1899 1909 1915 1927 1933 1970 1975 1980 1984 2004 2011\n",
      " 2016 2017 2019 2020 2040 2044 2049 2055 2076 2082 2100 2103 2107 2117\n",
      " 2128 2134 2147 2155 2156 2166 2168 2175 2189 2199 2215 2250 2271 2273\n",
      " 2292 2303 2316 2327 2330 2339 2354 2360 2366 2372 2373 2376 2385 2390\n",
      " 2415 2421 2426 2437 2465 2486 2487 2491 2492 2493 2494 2500 2510 2513\n",
      " 2520 2539 2567 2569 2577 2578 2582 2585 2586 2598 2600 2623 2629 2633\n",
      " 2638 2648 2649 2650 2651 2660 2678 2683 2691 2698 2700 2711 2712 2724\n",
      " 2748 2812 2817 2819 2849 2857 2868 2887 2922 2944 2951 2953 2959 2961\n",
      " 2982 3026 3030 3044 3048 3055 3067 3069 3079 3083 3093 3098 3103 3107\n",
      " 3108 3111 3125 3157 3178 3180 3202 3206 3228 3241 3260 3279 3294 3295\n",
      " 3296 3325 3331 3333 3343 3345 3354 3365 3385 3406 3420 3468 3472 3487\n",
      " 3496 3498 3500 3504 3507 3524 3525 3526 3564 3573 3578 3593 3618 3623\n",
      " 3639 3644 3668 3673 3685 3689 3729 3749 3772 3784 3798 3799 3819 3826\n",
      " 3850 3852 3895 3903 3918 3934 3949 3961 3969 3970 3979 3983 3995 4023\n",
      " 4026 4061 4081 4086 4105 4112 4116 4144 4148 4199 4207 4236 4237 4249\n",
      " 4260 4269 4271 4276 4308 4315 4330 4335 4349 4353 4355 4357 4361 4363\n",
      " 4364 4367 4369 4418 4419 4455 4459 4469 4470 4472 4481 4485 4498 4503\n",
      " 4539 4558 4586 4590 4593 4600 4602 4604 4607 4608 4617 4621 4622 4628\n",
      " 4640 4641 4642 4652 4670 4677 4686 4718 4722 4723 4731 4740 4748 4753\n",
      " 4764 4771 4779 4783 4791 4793 4796 4807 4808 4822 4855 4858 4863 4865\n",
      " 4880 4889 4890 4894 4926 4933 4943 4948 4980 4987 5011 5014 5018 5031\n",
      " 5045 5052 5074 5098 5108 5109 5119 5121 5132 5140 5166 5183 5195 5196\n",
      " 5204 5211 5230 5266 5276 5282 5292 5298 5303 5304 5339 5351 5352 5357\n",
      " 5381 5390 5393 5396 5398 5403 5437 5443 5456 5465 5497 5519 5521 5535\n",
      " 5556 5570 5586 5592 5598 5607 5608 5612 5613 5621 5623 5635 5641 5645\n",
      " 5669 5679 5694 5695 5696 5698 5712 5720 5731 5755 5777 5785 5791 5794\n",
      " 5826 5833 5840 5851 5854 5896 5910 5911 5917 5928 5937 5943 5948 5949\n",
      " 5953 5961 5981 5994 5997 6000 6006 6023 6029 6043 6053 6079 6086 6090\n",
      " 6092 6104 6106 6107 6111 6116 6132 6149 6162 6170 6191 6194 6195 6199\n",
      " 6225 6229 6238 6249 6253 6268 6279 6280 6311 6313 6315 6317 6319 6336\n",
      " 6344 6349 6367 6390 6392 6395 6398 6404 6407 6432 6445 6446 6468 6476\n",
      " 6479 6517 6531 6539 6543 6559 6560 6561 6565 6594 6595 6598 6657 6664\n",
      " 6665 6671 6674 6680 6688 6692 6693 6711 6721 6724 6726 6730 6740 6743\n",
      " 6749 6756 6757 6765 6773 6786 6799 6800 6811 6818 6824 6836 6839 6841\n",
      " 6856 6857 6858 6867 6872 6886 6893 6899 6925 6932]\n",
      "\n",
      "[   0    1    2 ... 6957 6959 6960]\n",
      "[   9   18   20   24   36   46   48   61   63   86   90   93   95  106\n",
      "  116  125  127  145  150  152  159  166  169  221  244  252  280  282\n",
      "  284  290  298  301  304  335  342  343  360  368  372  373  388  390\n",
      "  393  404  408  422  423  432  458  464  473  479  487  496  503  511\n",
      "  512  514  517  518  529  544  546  554  587  588  591  607  611  612\n",
      "  625  652  667  668  670  672  680  706  724  726  747  755  783  790\n",
      "  798  802  806  809  824  825  827  828  833  843  845  847  864  871\n",
      "  877  889  927  929  939  951  956  963  984  987  994 1001 1004 1023\n",
      " 1024 1028 1059 1075 1080 1092 1110 1138 1152 1156 1166 1171 1173 1186\n",
      " 1208 1217 1224 1241 1247 1249 1259 1280 1284 1293 1304 1318 1321 1334\n",
      " 1340 1353 1363 1365 1370 1379 1380 1392 1395 1405 1418 1419 1436 1440\n",
      " 1443 1445 1456 1462 1469 1499 1511 1512 1523 1535 1550 1555 1566 1581\n",
      " 1601 1623 1625 1626 1642 1673 1688 1706 1710 1712 1719 1731 1732 1736\n",
      " 1740 1746 1748 1797 1804 1813 1814 1816 1820 1846 1856 1865 1874 1882\n",
      " 1889 1897 1934 1943 1949 1950 1969 1974 1981 1993 1998 1999 2009 2014\n",
      " 2030 2077 2084 2094 2114 2123 2132 2145 2149 2152 2162 2164 2181 2192\n",
      " 2208 2246 2249 2253 2257 2261 2278 2300 2309 2322 2324 2325 2335 2350\n",
      " 2353 2397 2398 2416 2423 2432 2454 2459 2464 2474 2508 2512 2538 2547\n",
      " 2557 2563 2574 2579 2580 2587 2599 2618 2627 2662 2669 2685 2710 2713\n",
      " 2728 2739 2756 2761 2769 2774 2775 2780 2805 2809 2822 2843 2844 2845\n",
      " 2854 2863 2871 2884 2885 2909 2916 2917 2925 2933 2947 2950 2968 2971\n",
      " 2986 2987 3006 3012 3025 3027 3043 3058 3060 3062 3077 3087 3089 3101\n",
      " 3105 3124 3132 3134 3141 3154 3156 3168 3176 3179 3185 3196 3197 3216\n",
      " 3237 3239 3278 3286 3287 3288 3292 3293 3302 3326 3330 3336 3348 3361\n",
      " 3369 3371 3388 3404 3410 3423 3429 3440 3445 3458 3474 3480 3486 3488\n",
      " 3528 3535 3541 3550 3561 3577 3579 3587 3588 3592 3600 3613 3617 3624\n",
      " 3629 3632 3646 3652 3655 3675 3677 3697 3702 3711 3714 3735 3736 3737\n",
      " 3738 3744 3750 3751 3766 3789 3790 3796 3797 3800 3805 3815 3817 3846\n",
      " 3847 3863 3869 3890 3902 3925 3931 3935 3937 3973 3976 3981 4003 4031\n",
      " 4033 4047 4048 4050 4065 4071 4073 4102 4117 4130 4141 4150 4156 4157\n",
      " 4160 4161 4165 4175 4197 4213 4221 4223 4253 4259 4270 4272 4297 4305\n",
      " 4339 4345 4347 4356 4360 4370 4376 4387 4412 4430 4435 4439 4446 4451\n",
      " 4462 4465 4467 4475 4478 4480 4489 4491 4513 4529 4540 4549 4569 4573\n",
      " 4576 4579 4588 4610 4625 4637 4648 4650 4664 4674 4678 4681 4697 4702\n",
      " 4706 4712 4715 4726 4728 4729 4736 4741 4752 4756 4778 4780 4794 4802\n",
      " 4810 4826 4833 4854 4869 4886 4887 4900 4905 4920 4923 4935 4942 4951\n",
      " 4956 4958 4977 4979 5009 5028 5057 5062 5065 5078 5079 5082 5086 5089\n",
      " 5097 5101 5103 5107 5124 5141 5143 5146 5147 5152 5158 5177 5179 5180\n",
      " 5184 5185 5188 5194 5225 5231 5232 5236 5246 5248 5250 5262 5264 5270\n",
      " 5283 5309 5316 5341 5344 5348 5367 5369 5397 5404 5411 5433 5442 5451\n",
      " 5461 5467 5472 5473 5504 5511 5520 5539 5540 5547 5575 5595 5601 5639\n",
      " 5660 5664 5676 5678 5692 5705 5742 5744 5753 5756 5786 5787 5801 5806\n",
      " 5829 5847 5884 5898 5907 5939 5941 5946 5993 6018 6024 6036 6038 6039\n",
      " 6044 6058 6065 6081 6085 6096 6105 6130 6155 6171 6175 6187 6193 6200\n",
      " 6213 6214 6221 6252 6261 6263 6267 6271 6272 6276 6285 6290 6297 6330\n",
      " 6337 6346 6350 6352 6366 6371 6375 6380 6396 6401 6437 6467 6482 6495\n",
      " 6499 6504 6507 6526 6527 6535 6538 6552 6562 6563 6578 6603 6618 6629\n",
      " 6637 6647 6655 6660 6668 6672 6675 6683 6686 6690 6695 6705 6737 6741\n",
      " 6745 6751 6752 6753 6761 6798 6814 6831 6844 6845 6847 6862 6881 6894\n",
      " 6901 6902 6920 6927 6928 6941 6944 6948 6949 6958]\n",
      "\n",
      "[   0    1    2 ... 6958 6959 6960]\n",
      "[  35   42   51   66   68   78   82  109  112  114  119  120  139  144\n",
      "  148  162  171  172  184  186  193  197  215  217  218  222  247  248\n",
      "  257  261  285  295  302  311  325  329  330  351  353  378  379  383\n",
      "  395  401  414  420  457  461  475  477  478  490  493  497  509  520\n",
      "  542  559  583  618  633  634  647  658  673  674  691  698  707  714\n",
      "  732  748  766  785  820  831  837  848  855  859  861  865  880  888\n",
      "  890  900  910  917  918  924  928  933  936  942  950  955  959  970\n",
      "  973  981  985  992  997  998 1009 1027 1056 1063 1072 1085 1098 1103\n",
      " 1118 1120 1134 1192 1211 1216 1235 1238 1294 1297 1307 1316 1329 1371\n",
      " 1387 1398 1410 1430 1433 1435 1437 1442 1454 1466 1477 1478 1500 1537\n",
      " 1541 1544 1561 1567 1571 1588 1595 1596 1609 1616 1618 1622 1649 1650\n",
      " 1657 1684 1692 1694 1697 1744 1747 1771 1780 1782 1784 1792 1802 1830\n",
      " 1842 1843 1848 1866 1893 1911 1924 1932 1939 1960 1967 1968 1972 1978\n",
      " 1986 1995 2003 2012 2023 2033 2039 2058 2060 2071 2072 2096 2101 2105\n",
      " 2112 2141 2142 2150 2163 2180 2193 2195 2197 2204 2212 2224 2228 2265\n",
      " 2285 2293 2296 2305 2306 2310 2311 2315 2320 2321 2329 2331 2333 2355\n",
      " 2363 2365 2392 2419 2422 2431 2439 2451 2453 2460 2463 2475 2480 2482\n",
      " 2506 2526 2545 2549 2555 2560 2581 2584 2613 2619 2622 2634 2645 2658\n",
      " 2671 2680 2701 2717 2721 2723 2742 2772 2786 2813 2823 2829 2835 2853\n",
      " 2883 2894 2897 2901 2926 2948 2949 2958 2970 2984 2988 2996 3007 3023\n",
      " 3057 3066 3078 3094 3095 3104 3110 3115 3136 3146 3149 3150 3152 3164\n",
      " 3166 3181 3191 3200 3205 3209 3210 3230 3242 3253 3259 3265 3284 3289\n",
      " 3304 3308 3324 3329 3337 3355 3356 3366 3377 3383 3384 3395 3421 3434\n",
      " 3436 3437 3442 3446 3448 3484 3499 3503 3506 3534 3540 3544 3549 3556\n",
      " 3568 3603 3645 3659 3667 3681 3683 3707 3718 3725 3728 3730 3732 3733\n",
      " 3762 3769 3783 3785 3786 3814 3820 3824 3831 3832 3834 3838 3841 3853\n",
      " 3867 3874 3881 3888 3896 3921 3922 3928 3932 3982 3989 3998 4005 4008\n",
      " 4016 4019 4020 4028 4041 4049 4082 4087 4092 4101 4104 4113 4126 4135\n",
      " 4153 4162 4171 4176 4177 4218 4219 4228 4229 4230 4233 4235 4240 4242\n",
      " 4243 4244 4256 4257 4267 4277 4284 4285 4296 4301 4312 4319 4328 4329\n",
      " 4346 4348 4354 4366 4382 4385 4390 4404 4413 4416 4421 4434 4442 4443\n",
      " 4468 4477 4483 4497 4501 4506 4508 4522 4524 4545 4547 4555 4568 4570\n",
      " 4591 4592 4597 4598 4609 4612 4619 4638 4643 4653 4654 4659 4666 4682\n",
      " 4707 4732 4757 4763 4773 4781 4795 4799 4823 4825 4842 4843 4853 4856\n",
      " 4888 4892 4922 4927 4940 4952 4957 4960 4965 4975 4986 4996 4997 5002\n",
      " 5005 5020 5024 5026 5027 5033 5073 5123 5128 5131 5142 5144 5150 5160\n",
      " 5162 5174 5178 5187 5189 5214 5224 5228 5237 5239 5242 5263 5268 5277\n",
      " 5300 5314 5315 5322 5342 5343 5345 5362 5364 5375 5382 5386 5387 5421\n",
      " 5422 5436 5457 5458 5460 5468 5475 5491 5498 5500 5502 5512 5515 5522\n",
      " 5545 5546 5549 5557 5563 5565 5574 5585 5604 5605 5618 5634 5649 5650\n",
      " 5657 5673 5677 5682 5683 5687 5714 5716 5717 5727 5733 5737 5739 5748\n",
      " 5754 5761 5762 5766 5769 5790 5820 5824 5836 5858 5862 5864 5866 5878\n",
      " 5893 5894 5912 5920 5923 5925 5927 5931 5935 5945 5947 5955 5959 5977\n",
      " 5978 5980 5987 6041 6045 6093 6102 6109 6120 6124 6125 6150 6176 6186\n",
      " 6189 6232 6256 6270 6273 6274 6277 6282 6296 6298 6304 6309 6310 6321\n",
      " 6325 6335 6360 6373 6377 6378 6379 6387 6391 6397 6399 6416 6424 6434\n",
      " 6435 6441 6442 6466 6472 6481 6490 6513 6521 6532 6541 6546 6550 6566\n",
      " 6570 6576 6581 6590 6613 6625 6626 6633 6636 6640 6651 6670 6702 6709\n",
      " 6713 6715 6722 6727 6728 6746 6748 6755 6780 6781 6782 6797 6820 6828\n",
      " 6832 6860 6884 6887 6911 6916 6922 6938 6939 6942]\n",
      "\n",
      "[   0    1    2 ... 6958 6959 6960]\n",
      "[  10   19   21   33   49   57   59   79   99  103  108  128  147  157\n",
      "  160  173  181  195  203  204  207  209  210  219  223  241  278  289\n",
      "  296  313  317  336  355  356  357  361  370  389  394  411  417  418\n",
      "  434  447  466  468  471  515  522  523  526  533  549  576  581  586\n",
      "  590  594  602  614  632  639  656  659  666  671  675  678  685  687\n",
      "  699  712  725  740  743  763  775  781  786  787  801  805  815  834\n",
      "  840  851  854  868  892  915  920  935  946  953  966  979  989 1000\n",
      " 1005 1016 1035 1038 1039 1041 1050 1062 1066 1078 1084 1106 1113 1144\n",
      " 1146 1147 1151 1154 1160 1164 1165 1169 1176 1177 1178 1190 1195 1197\n",
      " 1220 1243 1258 1267 1269 1277 1290 1295 1299 1317 1322 1335 1346 1357\n",
      " 1358 1369 1375 1393 1408 1417 1432 1434 1438 1465 1468 1476 1488 1491\n",
      " 1495 1502 1507 1517 1522 1530 1538 1572 1593 1598 1602 1606 1607 1612\n",
      " 1614 1617 1630 1634 1666 1683 1699 1715 1716 1730 1734 1760 1766 1770\n",
      " 1773 1785 1786 1793 1810 1824 1837 1863 1870 1873 1879 1892 1896 1904\n",
      " 1913 1926 1941 1942 1953 1954 1955 1958 1961 1965 1971 1989 2002 2022\n",
      " 2024 2025 2031 2053 2085 2086 2092 2095 2113 2119 2122 2137 2140 2172\n",
      " 2182 2187 2200 2201 2206 2210 2219 2242 2244 2252 2254 2259 2260 2267\n",
      " 2274 2289 2328 2375 2378 2379 2389 2395 2401 2408 2430 2446 2452 2456\n",
      " 2468 2476 2478 2488 2503 2507 2509 2515 2517 2527 2530 2542 2544 2546\n",
      " 2552 2568 2606 2616 2630 2636 2639 2661 2667 2670 2682 2699 2704 2706\n",
      " 2718 2722 2727 2732 2733 2736 2740 2749 2759 2771 2773 2777 2782 2814\n",
      " 2834 2850 2861 2870 2879 2882 2892 2896 2910 2915 2939 2941 2973 2978\n",
      " 2985 3000 3022 3033 3037 3038 3047 3051 3068 3090 3091 3097 3118 3127\n",
      " 3144 3162 3163 3169 3182 3190 3198 3212 3215 3225 3236 3264 3285 3290\n",
      " 3299 3300 3312 3314 3315 3318 3359 3382 3386 3392 3408 3411 3455 3469\n",
      " 3481 3482 3495 3497 3537 3548 3554 3562 3563 3569 3574 3589 3590 3594\n",
      " 3597 3599 3609 3612 3634 3650 3662 3664 3679 3680 3687 3694 3712 3716\n",
      " 3743 3752 3754 3757 3770 3778 3781 3788 3821 3828 3833 3845 3848 3873\n",
      " 3879 3880 3884 3893 3936 3953 3965 3977 3978 3992 4004 4006 4007 4009\n",
      " 4011 4013 4058 4090 4110 4131 4133 4138 4143 4158 4183 4196 4208 4238\n",
      " 4239 4246 4252 4258 4268 4275 4291 4293 4295 4309 4311 4317 4322 4323\n",
      " 4326 4340 4358 4374 4391 4397 4428 4450 4453 4454 4473 4486 4512 4530\n",
      " 4533 4560 4562 4574 4584 4618 4632 4660 4661 4667 4668 4691 4694 4701\n",
      " 4711 4737 4739 4744 4758 4760 4775 4784 4797 4798 4815 4818 4835 4838\n",
      " 4840 4852 4861 4901 4902 4903 4914 4945 4954 4985 4989 4992 5004 5008\n",
      " 5013 5016 5043 5044 5049 5050 5058 5059 5090 5105 5154 5161 5165 5192\n",
      " 5193 5198 5205 5209 5217 5227 5244 5251 5256 5259 5287 5323 5331 5354\n",
      " 5359 5370 5372 5376 5378 5400 5413 5423 5425 5427 5441 5480 5496 5506\n",
      " 5517 5527 5560 5568 5572 5596 5597 5624 5633 5636 5637 5646 5647 5655\n",
      " 5668 5681 5685 5706 5711 5721 5726 5749 5771 5772 5778 5783 5803 5807\n",
      " 5810 5813 5830 5832 5856 5865 5871 5876 5879 5886 5888 5897 5914 5921\n",
      " 5922 5938 5950 5951 5954 5962 5967 5970 5986 6008 6009 6016 6022 6025\n",
      " 6026 6060 6061 6064 6069 6077 6091 6097 6103 6115 6135 6140 6145 6164\n",
      " 6165 6167 6179 6182 6202 6206 6211 6219 6226 6243 6244 6278 6293 6300\n",
      " 6301 6322 6334 6339 6343 6385 6393 6403 6408 6414 6439 6452 6456 6460\n",
      " 6474 6478 6488 6489 6497 6509 6512 6514 6518 6519 6520 6534 6545 6553\n",
      " 6569 6575 6585 6588 6599 6608 6609 6610 6612 6614 6619 6620 6624 6632\n",
      " 6659 6663 6684 6685 6697 6699 6701 6703 6707 6712 6719 6739 6744 6760\n",
      " 6768 6769 6795 6803 6805 6810 6826 6851 6854 6866 6868 6871 6876 6877\n",
      " 6892 6904 6910 6915 6917 6923 6930 6934 6936 6937]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthurn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arthurn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/arthurn/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# # print(val_texts, val_labels)\n",
    "\n",
    "# for train_index, val_index in kf.split(val_texts, val_labels):\n",
    "#     print(train_index)\n",
    "#     print(val_index)\n",
    "#     print()\n",
    "# #     # splitting Dataframe (dataset not included)\n",
    "# #     train_df = train_data.iloc[train_index]\n",
    "# #     val_df = train_data.iloc[val_index]\n",
    "# #     # Defining Model\n",
    "# #     model = ClassificationModel('bert', 'bert-base-uncased') \n",
    "# #     # train the model\n",
    "# #     model.train_model(train_df)\n",
    "# #     # validate the model \n",
    "# #     result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
    "# #     print(result['acc'])\n",
    "# #     # append model score\n",
    "# #     results.append(result['acc'])\n",
    "\n",
    "# # print(cross_val_score(LogisticRegressionCV(random_state=42), x_train, y_train, cv=10, verbose=1, n_jobs=-1, scoring='recall').mean())\n",
    "\n",
    "\n",
    "# print(classification_report([1, 0], [1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c0cd3ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 6 into shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9690/398395799.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 6 into shape (2,)"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.concatenate(([1, 2, 3], [4, 5, 6])).reshape((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01289893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertos: 270\n",
      "Total: 276\n",
      "Acurácia: 97.8261%\n",
      "Precision: 0.98%\n",
      "Recall: 0.98%\n",
      "Fscore: 0.98%\n"
     ]
    }
   ],
   "source": [
    "# Descobrindo acurácia do Modelo\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "cm = {'true_positive': 0, 'true_negative': 0, 'false_positive': 0, 'false_negative': 0}\n",
    "for i in range(len(val_texts)):\n",
    "    tokenized_text = tokenizer(val_texts[i], truncation=True, padding=True, max_length=512)\n",
    "    # print(entry)\n",
    "    output = model(torch.tensor([tokenized_text.input_ids]))\n",
    "    if (torch.argmax(output.logits, dim=-1) == val_labels[i]):\n",
    "        if val_labels[i] == 1:\n",
    "            cm['true_positive'] += 1\n",
    "        else:\n",
    "            cm['true_negative'] += 1\n",
    "    else:\n",
    "        if val_labels[i] == 0:\n",
    "            cm['false_positive'] += 1\n",
    "        else:\n",
    "            cm['false_negative'] += 1\n",
    "        \n",
    "precision = cm['true_positive'] / (cm['true_positive'] + cm['false_positive'])\n",
    "recall = cm['true_positive'] / (cm['true_positive'] + cm['false_negative'])\n",
    "fscore = 2 * ((precision * recall) / (precision + recall))\n",
    "acertos = cm['true_positive'] + cm['true_negative']\n",
    "print(f'Acertos: {cm[\"true_positive\"] + cm[\"true_negative\"]}')\n",
    "print(f'Total: {len(val_texts)}')\n",
    "print(f'Acurácia: {(acertos / len(val_texts)) * 100:.4f}%')\n",
    "print(f'Precision: {precision:.4f}%')\n",
    "print(f'Recall: {recall:.4f}%')\n",
    "print(f'Fscore: {fscore:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24a164c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Predição de textos específicos\n",
    "\n",
    "text = 'Texto aqui'\n",
    "\n",
    "tokenized_text = tokenizer(text, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "output = model(torch.tensor([tokenized_text.input_ids]))\n",
    "\n",
    "print(torch.argmax(output.logits, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dab1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
